[
  {
    "global_chunk_id": "40719b86-5921-456c-a648-f07a4e4057aa",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_0",
    "chunk_index": 0,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 0,
    "bbox": [
      116,
      130,
      325,
      147
    ],
    "text": "Academic Year: 2025-26",
    "raw_text": "Academic Year: 2025-26",
    "section_hierarchy": "St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 22,
    "word_count": 3,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.354603",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303aa"
  },
  {
    "global_chunk_id": "1fd37c11-99b1-42c0-8936-99b342555866",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_1",
    "chunk_index": 1,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 0,
    "bbox": [
      393,
      128,
      633,
      147
    ],
    "text": "Class-BE( INFT) Div- A &B",
    "raw_text": "Class-BE(INFT) Div- A &B",
    "section_hierarchy": "St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 25,
    "word_count": 5,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.354603",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303ab"
  },
  {
    "global_chunk_id": "bf5bfca0-59e6-4e26-8292-7e71a2cd9a32",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_2",
    "chunk_index": 2,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 0,
    "bbox": [
      114,
      347,
      885,
      406
    ],
    "text": "Special cells in the eye's retina that are responsible for converting light into signals that are sent to the brain. Photoreceptors give us our color vision and night vision. There are two types of photoreceptor cells: rods and cones. A number of eye problems can involve photoreceptor cells.",
    "raw_text": "Special cells in the eye's retina that are responsible for converting light into signals that are sent to the brain. Photoreceptors give us our color vision and night vision. There are two types of photoreceptor cells: rods and cones. A number of eye problems can involve photoreceptor cells.",
    "section_hierarchy": "Module6: Visual Physiology, perception and tracking > 6. 1. Functioning of Eye with photoreceptors > 6. 1. 1 Definition",
    "heading_context": "6. 1. 1 Definition",
    "num_merged_blocks": 1,
    "char_count": 292,
    "word_count": 48,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.354603",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303ac"
  },
  {
    "global_chunk_id": "6221d9d2-4142-4713-8ae5-4acdb229eac7",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_3",
    "chunk_index": 3,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 0,
    "bbox": [
      116,
      819,
      883,
      878
    ],
    "text": "The above figure shows the physiology ofahuman eye. The shape is approximately spherical, with a diameter of around $2 4 \\mathrm { m m }$ and only slight variation among people. The cornea isahard, ansparent surface through which light enters and provides the greatest optical power.",
    "raw_text": "The above figure shows the physiology of a human eye. The shape is approximately spherical, with a diameter of around $2 4 \\mathrm { m m }$ and only slight variation among people. The cornea is a hard, ansparent surface through which light enters and provides the greatest optical power.",
    "section_hierarchy": "Module6: Visual Physiology, perception and tracking > 6. 1. Functioning of Eye with photoreceptors > 6. 1. 1 Definition",
    "heading_context": "6. 1. 1 Definition",
    "num_merged_blocks": 1,
    "char_count": 283,
    "word_count": 46,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.355470",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303ad"
  },
  {
    "global_chunk_id": "c0edec76-813b-4609-b658-c15635235667",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_4",
    "chunk_index": 4,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 1,
    "bbox": [
      116,
      130,
      325,
      147
    ],
    "text": "Academic Year: 2025-26",
    "raw_text": "Academic Year: 2025-26",
    "section_hierarchy": "6. 1. Functioning of Eye with photoreceptors > 6. 1. 1 Definition > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 22,
    "word_count": 3,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.355470",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303ae"
  },
  {
    "global_chunk_id": "f31b15b4-77a9-4bfa-9408-bd8e7137b7a8",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_5",
    "chunk_index": 5,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 1,
    "bbox": [
      393,
      128,
      633,
      147
    ],
    "text": "Class-BE( INFT) Div- A &B",
    "raw_text": "Class-BE(INFT) Div- A &B",
    "section_hierarchy": "6. 1. Functioning of Eye with photoreceptors > 6. 1. 1 Definition > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 25,
    "word_count": 5,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.355980",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303af"
  },
  {
    "global_chunk_id": "6527a4f3-76a0-4846-98fb-65941ac04683",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_6",
    "chunk_index": 6,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 1,
    "bbox": [
      114,
      227,
      883,
      325
    ],
    "text": "The rest of the outer surface of the eye is protected byahard, white layer called the sclera. Mofthe eye interior consists of vitreous humor, which isatransparent, gelatinous mass that allo light rays to penetrate with little distortion or attenuation. As light rays cross the cornea, they p through a small chamber containing aqueous humour, which is another transparent, gelatinous mass.",
    "raw_text": "The rest of the outer surface of the eye is protected by a hard, white layer called the sclera. M of the eye interior consists of vitreous humor, which is a transparent, gelatinous mass that allo light rays to penetrate with little distortion or attenuation. As light rays cross the cornea, they p through a small chamber containing aqueous humour, which is another transparent, gelatinous mass.",
    "section_hierarchy": "6. 1. Functioning of Eye with photoreceptors > 6. 1. 1 Definition > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 389,
    "word_count": 60,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.355980",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303b0"
  },
  {
    "global_chunk_id": "c137e199-2e4e-4f11-8ddf-ea9c3f6643d6",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_7",
    "chunk_index": 7,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 1,
    "bbox": [
      116,
      349,
      882,
      409
    ],
    "text": "After crossing this, rays enter the lens by passing through the pupil. The size of the pupil is controlled byadisc-shaped structure called the iris, which provides an aperture that regulates the amount of light that is allowed to pass.",
    "raw_text": "After crossing this, rays enter the lens by passing through the pupil. The size of the pupil is controlled by a disc-shaped structure called the iris, which provides an aperture that regulates the amount of light that is allowed to pass.",
    "section_hierarchy": "6. 1. Functioning of Eye with photoreceptors > 6. 1. 1 Definition > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 235,
    "word_count": 39,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.355980",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303b1"
  },
  {
    "global_chunk_id": "0e6560d4-983d-4b36-87f1-1f0f8c6cd199",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_8",
    "chunk_index": 8,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 1,
    "bbox": [
      116,
      433,
      883,
      491
    ],
    "text": "The optical power of the lens is altered by ciliary muscles. After passing through the lens, rays pass through the vitreous humor and strike the retina, which lines more than $1 8 0 \\textdegree$ of the inner eye boundary.",
    "raw_text": "The optical power of the lens is altered by ciliary muscles. After passing through the lens, rays pass through the vitreous humor and strike the retina, which lines more than $1 8 0 \\textdegree$ of the inner eye boundary.",
    "section_hierarchy": "6. 1. Functioning of Eye with photoreceptors > 6. 1. 1 Definition > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 221,
    "word_count": 39,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.356993",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303b2"
  },
  {
    "global_chunk_id": "a84ded75-8e97-40a6-889d-b3c935bd0bdc",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_9",
    "chunk_index": 9,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 1,
    "bbox": [
      114,
      515,
      883,
      654
    ],
    "text": "Since Figure shows a 2D cross section, the retina is shaped like an arc; however, keep in mind that itisa 2D surface. Imagine itasa curved counterpart toavisual display. To catch the light from the output pixels, itislined with photoreceptors, which behave like “input pixels”. The most important part of the retina is the fovea; the highest visual acuity, which isameasure of the sharpness or clarity of vision, is provided for rays that land on it. The optic disc isasmall hole in the retina through which neural pulses are transmitted outside of the eye through the optic nerve. Itison the same side of the fovea as the nose.",
    "raw_text": "Since Figure shows a 2D cross section, the retina is shaped like an arc; however, keep in mind that it is a 2D surface. Imagine it as a curved counterpart to a visual display. To catch the light from the output pixels, it is lined with photoreceptors, which behave like “input pixels”. The most important part of the retina is the fovea; the highest visual acuity, which is a measure of the sharpness or clarity of vision, is provided for rays that land on it. The optic disc is a small hole in the retina through which neural pulses are transmitted outside of the eye through the optic nerve. It is on the same side of the fovea as the nose.",
    "section_hierarchy": "6. 1. Functioning of Eye with photoreceptors > 6. 1. 1 Definition > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 628,
    "word_count": 107,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.356993",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303b3"
  },
  {
    "global_chunk_id": "3ca36231-d5a7-4fc5-8ab9-b0228557bc61",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_10",
    "chunk_index": 10,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 1,
    "bbox": [
      114,
      679,
      883,
      737
    ],
    "text": "Photoreceptors The retina contains two kinds of photoreceptors for vision: 1) rods, which are triggered by very low levels of light, and 2) cones, which require more light and are designed to distinguish between colors.",
    "raw_text": "Photoreceptors The retina contains two kinds of photoreceptors for vision: 1) rods, which are triggered by very low levels of light, and 2) cones, which require more light and are designed to distinguish between colors.",
    "section_hierarchy": "6. 1. Functioning of Eye with photoreceptors > 6. 1. 1 Definition > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 219,
    "word_count": 35,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.357994",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303b4"
  },
  {
    "global_chunk_id": "ab09cfbe-1be4-4246-a4d3-bcc17007135d",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_11",
    "chunk_index": 11,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 2,
    "bbox": [
      116,
      130,
      325,
      147
    ],
    "text": "Academic Year: 2025-26",
    "raw_text": "Academic Year: 2025-26",
    "section_hierarchy": "6. 1. 1 Definition > St. Francis Institute of Technology Department of information Technology > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 22,
    "word_count": 3,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.357994",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303b5"
  },
  {
    "global_chunk_id": "e1d8f83f-3ca6-49c6-9858-8352371eb527",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_12",
    "chunk_index": 12,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 2,
    "bbox": [
      393,
      128,
      633,
      147
    ],
    "text": "Class-BE( INFT) Div- A &B",
    "raw_text": "Class-BE(INFT) Div- A &B",
    "section_hierarchy": "6. 1. 1 Definition > St. Francis Institute of Technology Department of information Technology > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 25,
    "word_count": 5,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.357994",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303b6"
  },
  {
    "global_chunk_id": "2bad4bf8-c51d-4765-8e0f-61b120b8b37e",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_13",
    "chunk_index": 13,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 2,
    "bbox": [
      116,
      526,
      883,
      606
    ],
    "text": "Figure shows the detection capabilities of each photoreceptor type. Rod sensitivity peaks at $4 9 8 \\mathrm { n m }$, between blue and green in the spectrum. Three categories of cones exist, based on whether they are designed to sense blue, green, or red light. Photoreceptors respond to light levels over a large dynamic range.",
    "raw_text": "Figure shows the detection capabilities of each photoreceptor type. Rod sensitivity peaks at $4 9 8 \\mathrm { n m }$ , between blue and green in the spectrum. Three categories of cones exist, based on whether they are designed to sense blue, green, or red light. Photoreceptors respond to light levels over a large dynamic range.",
    "section_hierarchy": "6. 1. 1 Definition > St. Francis Institute of Technology Department of information Technology > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 328,
    "word_count": 56,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.357994",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303b7"
  },
  {
    "global_chunk_id": "c00e7983-30c1-48f1-9bcd-ad475d877e28",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_14",
    "chunk_index": 14,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 2,
    "bbox": [
      114,
      670,
      883,
      789
    ],
    "text": "The density of photoreceptors across the retina varies greatly. The most interesting region is the fovea, which has the greatest concentration of photoreceptors. The innermost part of the fovea has a diameter of only $0. 5 \\mathrm { m m }$ oranangular range of $\\pm 0. 8 5$ degrees, and contains almost entirely cones. This implies that the eye must be pointed straight atatarget to perceive a sharp, colored image. The entire fovea has diameter $1. 5 \\mathrm { m m }$ $( \\pm 2. 6 $ degrees angular range), with the outer ring having a dominant concentration of rods.",
    "raw_text": "The density of photoreceptors across the retina varies greatly.The most interesting region is the fovea, which has the greatest concentration of photoreceptors. The innermost part of the fovea has a diameter of only $0 . 5 \\mathrm { m m }$ or an angular range of $\\pm 0 . 8 5$ degrees, and contains almost entirely cones. This implies that the eye must be pointed straight at a target to perceive a sharp, colored image. The entire fovea has diameter $1 . 5 \\mathrm { m m }$ $( \\pm 2 . 6 $ degrees angular range), with the outer ring having a dominant concentration of rods.",
    "section_hierarchy": "St. Francis Institute of Technology Department of information Technology > St. Francis Institute of Technology Department of information Technology > 6. 1. 2 Photoreceptor density",
    "heading_context": "6. 1. 2 Photoreceptor density",
    "num_merged_blocks": 1,
    "char_count": 567,
    "word_count": 100,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.357994",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303b8"
  },
  {
    "global_chunk_id": "6baa560b-6b1c-4631-9bb9-fac4c474cb75",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_15",
    "chunk_index": 15,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 2,
    "bbox": [
      116,
      791,
      883,
      869
    ],
    "text": "Rays that enter the cornea from the sides land on parts of the retina with lower rod density and very low cone density. This corresponds to the case of peripheral vision. We are much better at detecting movement in our periphery, but cannot distinguish colors effectively. Peripheral vement detection may have helped our ancestors from being eaten by predators. Finally, the",
    "raw_text": "Rays that enter the cornea from the sides land on parts of the retina with lower rod density and very low cone density. This corresponds to the case of peripheral vision. We are much better at detecting movement in our periphery, but cannot distinguish colors effectively. Peripheral vement detection may have helped our ancestors from being eaten by predators. Finally, the",
    "section_hierarchy": "St. Francis Institute of Technology Department of information Technology > St. Francis Institute of Technology Department of information Technology > 6. 1. 2 Photoreceptor density",
    "heading_context": "6. 1. 2 Photoreceptor density",
    "num_merged_blocks": 1,
    "char_count": 374,
    "word_count": 61,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.359503",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303b9"
  },
  {
    "global_chunk_id": "ffadeeb2-4db2-4d46-8332-dd6eefd91fbd",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_16",
    "chunk_index": 16,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 3,
    "bbox": [
      116,
      130,
      325,
      147
    ],
    "text": "Academic Year: 2025-26",
    "raw_text": "Academic Year: 2025-26",
    "section_hierarchy": "St. Francis Institute of Technology Department of information Technology > 6. 1. 2 Photoreceptor density > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 22,
    "word_count": 3,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.359503",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303ba"
  },
  {
    "global_chunk_id": "2fc81446-5c5d-4f06-a098-7f22fd758227",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_17",
    "chunk_index": 17,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 3,
    "bbox": [
      393,
      128,
      633,
      147
    ],
    "text": "Class-BE( INFT) Div- A &B",
    "raw_text": "Class-BE(INFT) Div- A &B",
    "section_hierarchy": "St. Francis Institute of Technology Department of information Technology > 6. 1. 2 Photoreceptor density > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 25,
    "word_count": 5,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.359503",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303bb"
  },
  {
    "global_chunk_id": "cfc560a5-4b0a-4d50-a084-2aff5ff7c74a",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_18",
    "chunk_index": 18,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 3,
    "bbox": [
      116,
      226,
      882,
      265
    ],
    "text": "most intriguing part of the plot is the blind spot, where there are no photoreceptors. This is due our retinas being inside-out and having no other way to route the neural signals to the brain.",
    "raw_text": "most intriguing part of the plot is the blind spot, where there are no photoreceptors. This is due our retinas being inside-out and having no other way to route the neural signals to the brain.",
    "section_hierarchy": "St. Francis Institute of Technology Department of information Technology > 6. 1. 2 Photoreceptor density > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 193,
    "word_count": 35,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.359503",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303bc"
  },
  {
    "global_chunk_id": "ba293a93-6f52-44be-8b06-e07d17de267e",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_19",
    "chunk_index": 19,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 3,
    "bbox": [
      116,
      318,
      883,
      377
    ],
    "text": "Resolution is defined as the number of pixels that can be displayed. The $1 9 2 0 \\mathrm { ~ x ~ } 1 0 8 0$ designation on your computer monitor is the device’s resolution. In this case, a landscape-orientation monitor is 1920 pixels wide and 1080 pixels high.",
    "raw_text": "Resolution is defined as the number of pixels that can be displayed. The $1 9 2 0 \\mathrm { ~ x ~ } 1 0 8 0$ designation on your computer monitor is the device’s resolution. In this case, a landscape-orientation monitor is 1920 pixels wide and 1080 pixels high.",
    "section_hierarchy": "6. 1. 2 Photoreceptor density > St. Francis Institute of Technology Department of information Technology > 6. 2 Resolution for VR",
    "heading_context": "6. 2 Resolution for VR",
    "num_merged_blocks": 1,
    "char_count": 261,
    "word_count": 50,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.360515",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303bd"
  },
  {
    "global_chunk_id": "40059832-f676-4a66-926b-76bb66f71310",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_20",
    "chunk_index": 20,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 3,
    "bbox": [
      116,
      401,
      882,
      502
    ],
    "text": "Perfect Resolution for VR: 8K per Eye Resolution isaphrase that extends beyond VR. Resolution is defined as the number of pixels that can be displayed. The $1 9 2 0 \\mathrm { ~ x ~ } 1 0 8 0$ designation on your computer monitor is the device’s resolution. In this case, a landscape-orientation monitor is 1920 pixels wide and 1080 pixels high.",
    "raw_text": "Perfect Resolution for VR: 8K per Eye Resolution is a phrase that extends beyond VR. Resolution is defined as the number of pixels that can be displayed. The $1 9 2 0 \\mathrm { ~ x ~ } 1 0 8 0$ designation on your computer monitor is the device’s resolution. In this case, a landscape-orientation monitor is 1920 pixels wide and 1080 pixels high.",
    "section_hierarchy": "6. 1. 2 Photoreceptor density > St. Francis Institute of Technology Department of information Technology > 6. 2 Resolution for VR",
    "heading_context": "6. 2 Resolution for VR",
    "num_merged_blocks": 2,
    "char_count": 344,
    "word_count": 63,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.360515",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303be"
  },
  {
    "global_chunk_id": "fbb289a0-e16f-4734-a2bc-5d1442f684ae",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_21",
    "chunk_index": 21,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 3,
    "bbox": [
      114,
      526,
      883,
      585
    ],
    "text": "On a 27-inch monitor, that might not seem like much. OnaVR screen measuring 3. 4 or 3. 6 inches, that isalot of pixel density. However there’satradeoff, in that your eyes’ proximity to those tiny screens means that greater resolution is needed to avoid the dreaded screen door effect.",
    "raw_text": "On a 27-inch monitor, that might not seem like much. On a VR screen measuring 3.4 or 3.6 inches, that is a lot of pixel density. However there’s a tradeoff, in that your eyes’ proximity to those tiny screens means that greater resolution is needed to avoid the dreaded screen door effect.",
    "section_hierarchy": "6. 1. 2 Photoreceptor density > St. Francis Institute of Technology Department of information Technology > 6. 2 Resolution for VR",
    "heading_context": "6. 2 Resolution for VR",
    "num_merged_blocks": 1,
    "char_count": 284,
    "word_count": 48,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.360515",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303bf"
  },
  {
    "global_chunk_id": "22ead95a-064e-4e78-b3fa-9196fd690bc5",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_22",
    "chunk_index": 22,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 3,
    "bbox": [
      114,
      609,
      883,
      728
    ],
    "text": "There are several components that go into creating a more immersive virtual experience. Some of the best VR headsets today offer around 210-degree fields of view, and high resolution spread across both eyes. Current standounts include the 2. 3 megapixel Valve Index $( 1, 4 4 0 \\mathrm { ~ x ~ } 1, 6 0 0$ pereye LCD) and the 4. 7 megapixel HP Reverb G2 $( 2, 1 6 0 \\mathrm { x } 2, 1 6 0$ per-eye LCD). Meanwhile, Varjo’s headsets have become the gold standard inVRresolution including the new consumer-geared Aero $\\left. 2, 8 8 0 \\mathrm { ~ x ~ } 2, 7 2 0 \\right.$ per-eye LCD).",
    "raw_text": "There are several components that go into creating a more immersive virtual experience. Some of the best VR headsets today offer around 210-degree fields of view, and high resolution spread across both eyes. Current standounts include the 2.3 megapixel Valve Index $( 1 , 4 4 0 \\mathrm { ~ x ~ } 1 , 6 0 0$ pereye LCD) and the 4.7 megapixel HP Reverb G2 $( 2 , 1 6 0 \\mathrm { x } 2 , 1 6 0$ per-eye LCD). Meanwhile, Varjo’s headsets have become the gold standard in VR resolution including the new consumer-geared Aero $\\left. 2 , 8 8 0 \\mathrm { ~ x ~ } 2 , 7 2 0 \\right.$ per-eye LCD).",
    "section_hierarchy": "6. 1. 2 Photoreceptor density > St. Francis Institute of Technology Department of information Technology > 6. 2 Resolution for VR",
    "heading_context": "6. 2 Resolution for VR",
    "num_merged_blocks": 1,
    "char_count": 582,
    "word_count": 114,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.360515",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303c0"
  },
  {
    "global_chunk_id": "19a17f7a-f084-4a8e-a827-afbaed46d1fb",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_23",
    "chunk_index": 23,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 3,
    "bbox": [
      116,
      835,
      883,
      893
    ],
    "text": "Eye rotations are a complicated and integral part of human vision. They occur both voluntarily and nvoluntarily, and allow a person to fixate on features in the world, even as his head or target eatures are moving.",
    "raw_text": "Eye rotations are a complicated and integral part of human vision. They occur both voluntarily and nvoluntarily, and allow a person to fixate on features in the world, even as his head or target eatures are moving.",
    "section_hierarchy": "6. 2 Resolution for VR > 6. 3 Eye movements and issues with itinVR > 6. 3. 1 Eye Movements",
    "heading_context": "6. 3. 1 Eye Movements",
    "num_merged_blocks": 1,
    "char_count": 214,
    "word_count": 37,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.362023",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303c1"
  },
  {
    "global_chunk_id": "5dd8bffd-1c31-4e79-a3b9-414a1e94ce5d",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_24",
    "chunk_index": 24,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 4,
    "bbox": [
      116,
      130,
      325,
      147
    ],
    "text": "Academic Year: 2025-26",
    "raw_text": "Academic Year: 2025-26",
    "section_hierarchy": "6. 3 Eye movements and issues with itinVR > 6. 3. 1 Eye Movements > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 22,
    "word_count": 3,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.362023",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303c2"
  },
  {
    "global_chunk_id": "9da70770-3247-42df-9de0-2dbda1f89b55",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_25",
    "chunk_index": 25,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 4,
    "bbox": [
      393,
      128,
      633,
      147
    ],
    "text": "Class-BE( INFT) Div- A &B",
    "raw_text": "Class-BE(INFT) Div- A &B",
    "section_hierarchy": "6. 3 Eye movements and issues with itinVR > 6. 3. 1 Eye Movements > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 25,
    "word_count": 5,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.362023",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303c3"
  },
  {
    "global_chunk_id": "ad7a870a-d3b8-449c-83fd-6f5765d0049e",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_26",
    "chunk_index": 26,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 4,
    "bbox": [
      114,
      227,
      883,
      306
    ],
    "text": "One of the main reasons for eye movement istoposition the feature of interest on the fovea. o the fovea can sense dense, color images, and it unfortunately spans a very narrow field of vie To gain a coherent, detailed view ofalarge object, the eyes rapidly scan over it while fixating points of interest.",
    "raw_text": "One of the main reasons for eye movement is to position the feature of interest on the fovea .o the fovea can sense dense, color images, and it unfortunately spans a very narrow field of vie To gain a coherent, detailed view of a large object, the eyes rapidly scan over it while fixating points of interest.",
    "section_hierarchy": "6. 3 Eye movements and issues with itinVR > 6. 3. 1 Eye Movements > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 304,
    "word_count": 53,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.363035",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303c4"
  },
  {
    "global_chunk_id": "4f49ab79-15a2-4da6-aab0-faa91fe18d55",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_27",
    "chunk_index": 27,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 4,
    "bbox": [
      116,
      308,
      883,
      364
    ],
    "text": "Another reason for eye movement is that our photoreceptors are slow to respond to stimuli due to their chemical nature. They take up to $1 0 \\mathrm { m s }$ to fully respond to stimuli and produce a response for up to $1 0 0 \\mathrm { m s }$.",
    "raw_text": "Another reason for eye movement is that our photoreceptors are slow to respond to stimuli due to their chemical nature. They take up to $1 0 \\mathrm { m s }$ to fully respond to stimuli and produce a response for up to $1 0 0 \\mathrm { m s }$ .",
    "section_hierarchy": "6. 3 Eye movements and issues with itinVR > 6. 3. 1 Eye Movements > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 243,
    "word_count": 51,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.363035",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303c5"
  },
  {
    "global_chunk_id": "a27eafd7-ec1a-4424-a6ea-4eb17b39279f",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_28",
    "chunk_index": 28,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 4,
    "bbox": [
      114,
      367,
      882,
      426
    ],
    "text": "Eye movements help keep the image fixed on the same set of photoreceptors so that they can fully charge. This is similar to the image blurring problem that occurs in cameras at low light levels and slow shutter speeds.",
    "raw_text": "Eye movements help keep the image fixed on the same set of photoreceptors so that they can fully charge. This is similar to the image blurring problem that occurs in cameras at low light levels and slow shutter speeds.",
    "section_hierarchy": "6. 3 Eye movements and issues with itinVR > 6. 3. 1 Eye Movements > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 218,
    "word_count": 39,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.364044",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303c6"
  },
  {
    "global_chunk_id": "e9abbdea-63cb-4fc4-883f-a52fbe305c49",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_29",
    "chunk_index": 29,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 4,
    "bbox": [
      114,
      428,
      883,
      526
    ],
    "text": "Additional reasons for eye movement are to maintain a stereoscopic view and to prevent adaptation toaconstant stimulation. To support the last claim, it has been shown experimentally that when eye motions are completely suppressed, visual perception disappears completely. As movements combine to build a coherent view, itisdifficult for scientists to predict and explain how people interpret some stimuli.",
    "raw_text": "Additional reasons for eye movement are to maintain a stereoscopic view and to prevent adaptation to a constant stimulation. To support the last claim, it has been shown experimentally that when eye motions are completely suppressed, visual perception disappears completely . As movements combine to build a coherent view, it is difficult for scientists to predict and explain how people interpret some stimuli.",
    "section_hierarchy": "6. 3 Eye movements and issues with itinVR > 6. 3. 1 Eye Movements > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 406,
    "word_count": 58,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.364044",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303c7"
  },
  {
    "global_chunk_id": "41f613b7-aee6-4fef-b722-e3f200491446",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_30",
    "chunk_index": 30,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 4,
    "bbox": [
      117,
      529,
      883,
      566
    ],
    "text": "Eye muscles The rotation of each eye is controlled by six muscles that are each attached to the sclera ( outer eyeball surface) byatendon. The tendons pull on the eye in opposite pairs.",
    "raw_text": "Eye muscles The rotation of each eye is controlled by six muscles that are each attached to the sclera (outer eyeball surface) by a tendon. The tendons pull on the eye in opposite pairs.",
    "section_hierarchy": "6. 3 Eye movements and issues with itinVR > 6. 3. 1 Eye Movements > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 185,
    "word_count": 33,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.364044",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303c8"
  },
  {
    "global_chunk_id": "3c157ea4-c801-4ae8-9784-61109cb5a070",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_31",
    "chunk_index": 31,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 4,
    "bbox": [
      114,
      568,
      883,
      887
    ],
    "text": "For example, to perform a yaw ( side-to-side) rotation, the tensions on the medial rectus and lateral rectus are varied while the other muscles are largely unaffected. To cause a pitch motion, four muscles per eye become involved. All six are involved to perform both a pitch and yaw, for example, looking upward and to the right. A small amount of roll can be generated; however, our eyes are generally not designed for much roll motion. Imagine if you could turn your eyeballs upside-down inside of their sockets! Thus, itisreasonable in most cases to approximate eye rotations as a 2D set that includes only yaw and pitch, rather than the full 3 DOFs obtained for rigid body rotations Types of movements We now consider movements based on their purpose, resulting in six categories: 1) saccades \n2) smooth pursuit \n3) vestibulo-ocular reflex,) optokinetic reflex,) vergence,",
    "raw_text": "For example, to perform a yaw (side-to-side) rotation, the tensions on the medial rectus and lateral rectus are varied while the other muscles are largely unaffected. To cause a pitch motion, four muscles per eye become involved. All six are involved to perform both a pitch and yaw, for example, looking upward and to the right. A small amount of roll can be generated; however, our eyes are generally not designed for much roll motion. Imagine if you could turn your eyeballs upside-down inside of their sockets! Thus, it is reasonable in most cases to approximate eye rotations as a 2D set that includes only yaw and pitch, rather than the full 3 DOFs obtained for rigid body rotations Types of movements We now consider movements based on their purpose, resulting in six categories: 1) saccades   \n2) smooth pursuit   \n3) vestibulo-ocular reflex,   \n) optokinetic reflex,   \n) vergence,",
    "section_hierarchy": "6. 3 Eye movements and issues with itinVR > 6. 3. 1 Eye Movements > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 3,
    "char_count": 877,
    "word_count": 144,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.366035",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303c9"
  },
  {
    "global_chunk_id": "b3a24d03-a77b-4fd6-a8cb-12f3a604993b",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_32",
    "chunk_index": 32,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 5,
    "bbox": [
      116,
      130,
      325,
      147
    ],
    "text": "Academic Year: 2025-26",
    "raw_text": "Academic Year: 2025-26",
    "section_hierarchy": "6. 3. 1 Eye Movements > St. Francis Institute of Technology Department of information Technology > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 22,
    "word_count": 3,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.366035",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303ca"
  },
  {
    "global_chunk_id": "d795095d-71fe-4abe-88ad-db5dc6b545f2",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_33",
    "chunk_index": 33,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 5,
    "bbox": [
      393,
      128,
      633,
      147
    ],
    "text": "Class-BE( INFT) Div- A &B",
    "raw_text": "Class-BE(INFT) Div- A &B",
    "section_hierarchy": "6. 3. 1 Eye Movements > St. Francis Institute of Technology Department of information Technology > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 25,
    "word_count": 5,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.366035",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303cb"
  },
  {
    "global_chunk_id": "ef906409-88cc-4e38-9532-4761cec9be74",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_34",
    "chunk_index": 34,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 5,
    "bbox": [
      114,
      268,
      880,
      306
    ],
    "text": "All of these motions cause both eyes to rotate approximately the same way, except for vergen which causes the eyes to rotate in opposite directions:",
    "raw_text": "All of these motions cause both eyes to rotate approximately the same way, except for vergen which causes the eyes to rotate in opposite directions:",
    "section_hierarchy": "St. Francis Institute of Technology Department of information Technology > St. Francis Institute of Technology Department of information Technology > 6) microsaccades.",
    "heading_context": "6) microsaccades.",
    "num_merged_blocks": 1,
    "char_count": 148,
    "word_count": 25,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.366035",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303cc"
  },
  {
    "global_chunk_id": "13778374-638e-418d-ac8b-55a42d6e3dc4",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_35",
    "chunk_index": 35,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 5,
    "bbox": [
      116,
      859,
      883,
      897
    ],
    "text": ". Optokinetic reflex -The next category is called the optokinetic reflex, which occurs when a fast bject speeds along. This occurs when watching a fastmoving train while standing nearby on fixed",
    "raw_text": ". Optokinetic reflex -The next category is called the optokinetic reflex, which occurs when a fast bject speeds along. This occurs when watching a fastmoving train while standing nearby on fixed",
    "section_hierarchy": "1. Saccades -The eye can move inarapid motion called a saccade, which lasts less than $4 5 \\mathrm { m s }$ with rotations of about $9 0 0 \\mathrm { { o } }$ per second. The purpose istoquickly relocate the fovea so that important features inascene are sensed with highest visual acuity. Each transition between features is accomplished byasaccade. Interestingly, our brains use saccadic masking to hide the intervals of time over which saccades occur from our memory. Although saccades frequently occur while we have little ornoawareness of them, we have the ability to consciously control them aswechoose features for fixation. > 2. Smooth pursuit -In the case of smooth pursuit, the eye slowly rotates to track a moving target feature. Examples are a car, a tennis ball, oraperson walking by. The rate of rotation is usually less than $3 0 \\textdegree$ per second, which is much slower than for saccades. The main function of smooth pursuit istoreduce motion blur on the retina; this is also known as image stabilization. The blur is due to the slow response time of photoreceptors, If the target is moving too fast, then saccades may be intermittently inserted into the pursuit motions to catch uptoit. > 3. Vestibulo-ocular reflex -One of the most important motions to understand for VRisthe vestibulo-ocular reflex or VOR. Hold your finger atacomfortable distance in front of your face and fixate on it. Next, yaw your head back and forth ( like you are nodding “no”), turning about 20 or 30 degrees to the left and right sides each time. You may notice that your eyes are effortlessly rotating to counteract the rotation of your head so that your finger remains in view. The eye motion is involuntary. If you do not believe it, then try to avoid rotating your eyes while paying attention to your finger and rotating your head. Itiscalled a reflex because the motion control bypasses higher brain functions. Based on angular accelerations sensed by vestibular organs, signals are sent to the eye muscles to provide the appropriate counter motion. The main purpose of the VOR istoprovide image stabilization, asinthe case of smooth pursuit.",
    "heading_context": "3. Vestibulo-ocular reflex -One of the most important motions to understand for VRisthe vestibulo-ocular reflex or VOR. Hold your finger atacomfortable distance in front of your face and fixate on it. Next, yaw your head back and forth ( like you are nodding “no”), turning about 20 or 30 degrees to the left and right sides each time. You may notice that your eyes are effortlessly rotating to counteract the rotation of your head so that your finger remains in view. The eye motion is involuntary. If you do not believe it, then try to avoid rotating your eyes while paying attention to your finger and rotating your head. Itiscalled a reflex because the motion control bypasses higher brain functions. Based on angular accelerations sensed by vestibular organs, signals are sent to the eye muscles to provide the appropriate counter motion. The main purpose of the VOR istoprovide image stabilization, asinthe case of smooth pursuit.",
    "num_merged_blocks": 1,
    "char_count": 194,
    "word_count": 31,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.369034",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303cd"
  },
  {
    "global_chunk_id": "7a24d030-9032-4d42-936f-7ca4183d04ab",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_36",
    "chunk_index": 36,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 6,
    "bbox": [
      116,
      130,
      325,
      147
    ],
    "text": "Academic Year: 2025-26",
    "raw_text": "Academic Year: 2025-26",
    "section_hierarchy": "2. Smooth pursuit -In the case of smooth pursuit, the eye slowly rotates to track a moving target feature. Examples are a car, a tennis ball, oraperson walking by. The rate of rotation is usually less than $3 0 \\textdegree$ per second, which is much slower than for saccades. The main function of smooth pursuit istoreduce motion blur on the retina; this is also known as image stabilization. The blur is due to the slow response time of photoreceptors, If the target is moving too fast, then saccades may be intermittently inserted into the pursuit motions to catch uptoit. > 3. Vestibulo-ocular reflex -One of the most important motions to understand for VRisthe vestibulo-ocular reflex or VOR. Hold your finger atacomfortable distance in front of your face and fixate on it. Next, yaw your head back and forth ( like you are nodding “no”), turning about 20 or 30 degrees to the left and right sides each time. You may notice that your eyes are effortlessly rotating to counteract the rotation of your head so that your finger remains in view. The eye motion is involuntary. If you do not believe it, then try to avoid rotating your eyes while paying attention to your finger and rotating your head. Itiscalled a reflex because the motion control bypasses higher brain functions. Based on angular accelerations sensed by vestibular organs, signals are sent to the eye muscles to provide the appropriate counter motion. The main purpose of the VOR istoprovide image stabilization, asinthe case of smooth pursuit. > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 22,
    "word_count": 3,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.369034",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303ce"
  },
  {
    "global_chunk_id": "90da1886-4800-460d-8e37-9f82c6afba81",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_37",
    "chunk_index": 37,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 6,
    "bbox": [
      393,
      128,
      633,
      147
    ],
    "text": "Class-BE( INFT) Div- A &B",
    "raw_text": "Class-BE(INFT) Div- A &B",
    "section_hierarchy": "2. Smooth pursuit -In the case of smooth pursuit, the eye slowly rotates to track a moving target feature. Examples are a car, a tennis ball, oraperson walking by. The rate of rotation is usually less than $3 0 \\textdegree$ per second, which is much slower than for saccades. The main function of smooth pursuit istoreduce motion blur on the retina; this is also known as image stabilization. The blur is due to the slow response time of photoreceptors, If the target is moving too fast, then saccades may be intermittently inserted into the pursuit motions to catch uptoit. > 3. Vestibulo-ocular reflex -One of the most important motions to understand for VRisthe vestibulo-ocular reflex or VOR. Hold your finger atacomfortable distance in front of your face and fixate on it. Next, yaw your head back and forth ( like you are nodding “no”), turning about 20 or 30 degrees to the left and right sides each time. You may notice that your eyes are effortlessly rotating to counteract the rotation of your head so that your finger remains in view. The eye motion is involuntary. If you do not believe it, then try to avoid rotating your eyes while paying attention to your finger and rotating your head. Itiscalled a reflex because the motion control bypasses higher brain functions. Based on angular accelerations sensed by vestibular organs, signals are sent to the eye muscles to provide the appropriate counter motion. The main purpose of the VOR istoprovide image stabilization, asinthe case of smooth pursuit. > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 25,
    "word_count": 5,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.369034",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303cf"
  },
  {
    "global_chunk_id": "ed59bfb4-daa8-4c17-99aa-4e152cd7bfba",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_38",
    "chunk_index": 38,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 6,
    "bbox": [
      116,
      227,
      882,
      265
    ],
    "text": "ground. The eyes rapidly and involuntarily choose features for tracking on the object, wh alternating between smooth pursuit and saccade motions.",
    "raw_text": "ground. The eyes rapidly and involuntarily choose features for tracking on the object, wh alternating between smooth pursuit and saccade motions.",
    "section_hierarchy": "2. Smooth pursuit -In the case of smooth pursuit, the eye slowly rotates to track a moving target feature. Examples are a car, a tennis ball, oraperson walking by. The rate of rotation is usually less than $3 0 \\textdegree$ per second, which is much slower than for saccades. The main function of smooth pursuit istoreduce motion blur on the retina; this is also known as image stabilization. The blur is due to the slow response time of photoreceptors, If the target is moving too fast, then saccades may be intermittently inserted into the pursuit motions to catch uptoit. > 3. Vestibulo-ocular reflex -One of the most important motions to understand for VRisthe vestibulo-ocular reflex or VOR. Hold your finger atacomfortable distance in front of your face and fixate on it. Next, yaw your head back and forth ( like you are nodding “no”), turning about 20 or 30 degrees to the left and right sides each time. You may notice that your eyes are effortlessly rotating to counteract the rotation of your head so that your finger remains in view. The eye motion is involuntary. If you do not believe it, then try to avoid rotating your eyes while paying attention to your finger and rotating your head. Itiscalled a reflex because the motion control bypasses higher brain functions. Based on angular accelerations sensed by vestibular organs, signals are sent to the eye muscles to provide the appropriate counter motion. The main purpose of the VOR istoprovide image stabilization, asinthe case of smooth pursuit. > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 145,
    "word_count": 21,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.370034",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303d0"
  },
  {
    "global_chunk_id": "b167f601-72dd-4f4a-b8da-229ca4db6029",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_39",
    "chunk_index": 39,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 6,
    "bbox": [
      114,
      474,
      883,
      573
    ],
    "text": "Eye Strain Eye strain is not unique toVRheadsets, but itisnonetheless a risk. Eye strain happens when the eyes are put through intense use. Itisa form of fatigue. Any eye discomfort caused by focusing the eyes onanobject ( or objects) for long periods of time would be considered eye strain, whether this happens while looking atacomputer screen, reading in poor visibility, driving for extended periods of time, or using a virtual reality headset.",
    "raw_text": "Eye Strain Eye strain is not unique to VR headsets, but it is nonetheless a risk. Eye strain happens when the eyes are put through intense use. It is a form of fatigue. Any eye discomfort caused by focusing the eyes on an object (or objects) for long periods of time would be considered eye strain, whether this happens while looking at a computer screen, reading in poor visibility, driving for extended periods of time, or using a virtual reality headset.",
    "section_hierarchy": "St. Francis Institute of Technology Department of information Technology > 5. Vergence -Stereopsis refers to the case in which both eyes are fixated on the same object, resulting inasingle perceived image. Two kinds of vergence motions occur to align the eyes with an object. If the object is closer than a previous fixation, then a convergence motion occurs. This means that the eyes are rotating so that the pupils are becoming closer. If the object is further, then divergence motion occurs, which causes the pupils to move further apart. The eye orientations resulting from vergence motions provide important information about the distance of objects. > 6. 3. 2 Issues inVRapplications due to eye movement",
    "heading_context": "6. 3. 2 Issues inVRapplications due to eye movement",
    "num_merged_blocks": 1,
    "char_count": 448,
    "word_count": 72,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.372207",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303d1"
  },
  {
    "global_chunk_id": "fb682aca-b646-4d16-a15b-dc8d91cc87cf",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_40",
    "chunk_index": 40,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 6,
    "bbox": [
      114,
      597,
      883,
      675
    ],
    "text": "Eye strain is known as asthenopia, and itisalso referred toaseye fatigue. Symptoms include tired eyes, which simply means difficulty with focusing vision, blurry vision, headaches, and sometimes double vision. These usually result from intense use of the eyes for visual tasks, often without adequate eye care and breaks.",
    "raw_text": "Eye strain is known as asthenopia, and it is also referred to as eye fatigue. Symptoms include tired eyes, which simply means difficulty with focusing vision, blurry vision, headaches, and sometimes double vision. These usually result from intense use of the eyes for visual tasks, often without adequate eye care and breaks.",
    "section_hierarchy": "St. Francis Institute of Technology Department of information Technology > 5. Vergence -Stereopsis refers to the case in which both eyes are fixated on the same object, resulting inasingle perceived image. Two kinds of vergence motions occur to align the eyes with an object. If the object is closer than a previous fixation, then a convergence motion occurs. This means that the eyes are rotating so that the pupils are becoming closer. If the object is further, then divergence motion occurs, which causes the pupils to move further apart. The eye orientations resulting from vergence motions provide important information about the distance of objects. > 6. 3. 2 Issues inVRapplications due to eye movement",
    "heading_context": "6. 3. 2 Issues inVRapplications due to eye movement",
    "num_merged_blocks": 1,
    "char_count": 321,
    "word_count": 48,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.372716",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303d2"
  },
  {
    "global_chunk_id": "678bf19c-9773-400e-95dd-4f3bf0849a9a",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_41",
    "chunk_index": 41,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 6,
    "bbox": [
      116,
      696,
      882,
      775
    ],
    "text": "Dry Eyes Most people tend to blink less than normal when carrying out visual tasks that require concentration, especially in gaming. A decrease in blinking can cause the surface of the eyes to become dry, which leads to eye pain and dry eyes. This is particularly true with aVRheadset, which can be both heavy and hot to use",
    "raw_text": "Dry Eyes Most people tend to blink less than normal when carrying out visual tasks that require concentration, especially in gaming. A decrease in blinking can cause the surface of the eyes to become dry, which leads to eye pain and dry eyes. This is particularly true with a VR headset, which can be both heavy and hot to use",
    "section_hierarchy": "St. Francis Institute of Technology Department of information Technology > 5. Vergence -Stereopsis refers to the case in which both eyes are fixated on the same object, resulting inasingle perceived image. Two kinds of vergence motions occur to align the eyes with an object. If the object is closer than a previous fixation, then a convergence motion occurs. This means that the eyes are rotating so that the pupils are becoming closer. If the object is further, then divergence motion occurs, which causes the pupils to move further apart. The eye orientations resulting from vergence motions provide important information about the distance of objects. > 6. 3. 2 Issues inVRapplications due to eye movement",
    "heading_context": "6. 3. 2 Issues inVRapplications due to eye movement",
    "num_merged_blocks": 1,
    "char_count": 324,
    "word_count": 58,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.372716",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303d3"
  },
  {
    "global_chunk_id": "1275df40-4cf8-4fcf-ab5f-7f75a19d965c",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_42",
    "chunk_index": 42,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 6,
    "bbox": [
      114,
      820,
      882,
      898
    ],
    "text": "Foveated rendering One of the frustrations with this analysis is that we have not been able to exploit that fact that photoreceptor density decreases away from the fovea. We had to keep the ixel density high everywhere because we have no control over which part of the display the user ill be look at. Ifwecould track where the eye is looking and have a tiny, movable display that is",
    "raw_text": "Foveated rendering One of the frustrations with this analysis is that we have not been able to exploit that fact that photoreceptor density decreases away from the fovea. We had to keep the ixel density high everywhere because we have no control over which part of the display the user ill be look at. If we could track where the eye is looking and have a tiny, movable display that is",
    "section_hierarchy": "St. Francis Institute of Technology Department of information Technology > 5. Vergence -Stereopsis refers to the case in which both eyes are fixated on the same object, resulting inasingle perceived image. Two kinds of vergence motions occur to align the eyes with an object. If the object is closer than a previous fixation, then a convergence motion occurs. This means that the eyes are rotating so that the pupils are becoming closer. If the object is further, then divergence motion occurs, which causes the pupils to move further apart. The eye orientations resulting from vergence motions provide important information about the distance of objects. > 6. 3. 2 Issues inVRapplications due to eye movement",
    "heading_context": "6. 3. 2 Issues inVRapplications due to eye movement",
    "num_merged_blocks": 1,
    "char_count": 383,
    "word_count": 69,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.373727",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303d4"
  },
  {
    "global_chunk_id": "0b24b2e1-17f8-4770-906c-1bd28152f9e8",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_43",
    "chunk_index": 43,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 7,
    "bbox": [
      116,
      130,
      325,
      147
    ],
    "text": "Academic Year: 2025-26",
    "raw_text": "Academic Year: 2025-26",
    "section_hierarchy": "5. Vergence -Stereopsis refers to the case in which both eyes are fixated on the same object, resulting inasingle perceived image. Two kinds of vergence motions occur to align the eyes with an object. If the object is closer than a previous fixation, then a convergence motion occurs. This means that the eyes are rotating so that the pupils are becoming closer. If the object is further, then divergence motion occurs, which causes the pupils to move further apart. The eye orientations resulting from vergence motions provide important information about the distance of objects. > 6. 3. 2 Issues inVRapplications due to eye movement > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 22,
    "word_count": 3,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.373727",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303d5"
  },
  {
    "global_chunk_id": "8af0b85c-1693-44cc-a552-10d9c2384dd6",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_44",
    "chunk_index": 44,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 7,
    "bbox": [
      393,
      128,
      633,
      147
    ],
    "text": "Class-BE( INFT) Div- A &B",
    "raw_text": "Class-BE(INFT) Div- A &B",
    "section_hierarchy": "5. Vergence -Stereopsis refers to the case in which both eyes are fixated on the same object, resulting inasingle perceived image. Two kinds of vergence motions occur to align the eyes with an object. If the object is closer than a previous fixation, then a convergence motion occurs. This means that the eyes are rotating so that the pupils are becoming closer. If the object is further, then divergence motion occurs, which causes the pupils to move further apart. The eye orientations resulting from vergence motions provide important information about the distance of objects. > 6. 3. 2 Issues inVRapplications due to eye movement > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 25,
    "word_count": 5,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.373727",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303d6"
  },
  {
    "global_chunk_id": "76154631-d07d-4db4-b370-eaaf328b384e",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_45",
    "chunk_index": 45,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 7,
    "bbox": [
      114,
      227,
      883,
      366
    ],
    "text": "always positioned in front of the pupil, with zero delay, then much fewer pixels would be need This would greatly decrease computational burdens on graphical rendering systems Instead moving a tiny screen, the process can be simulated by keeping the fixed display but focusing graphical rendering only in the spot where the eye is looking. This is called foveated rendering, which has been shown to work [ 106], but is currently too costly and there is too much delay and other discrepancies between the eye movements and the display updates. In the near future, it may become an effective approach for the mass market.",
    "raw_text": "always positioned in front of the pupil, with zero delay, then much fewer pixels would be need This would greatly decrease computational burdens on graphical rendering systems Instead moving a tiny screen, the process can be simulated by keeping the fixed display but focusing graphical rendering only in the spot where the eye is looking. This is called foveated rendering, which has been shown to work [106], but is currently too costly and there is too much delay and other discrepancies between the eye movements and the display updates. In the near future, it may become an effective approach for the mass market.",
    "section_hierarchy": "5. Vergence -Stereopsis refers to the case in which both eyes are fixated on the same object, resulting inasingle perceived image. Two kinds of vergence motions occur to align the eyes with an object. If the object is closer than a previous fixation, then a convergence motion occurs. This means that the eyes are rotating so that the pupils are becoming closer. If the object is further, then divergence motion occurs, which causes the pupils to move further apart. The eye orientations resulting from vergence motions provide important information about the distance of objects. > 6. 3. 2 Issues inVRapplications due to eye movement > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 619,
    "word_count": 104,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.374732",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303d7"
  },
  {
    "global_chunk_id": "030c5461-52f5-4fd9-bb39-e07c7609ac13",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_46",
    "chunk_index": 46,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 7,
    "bbox": [
      114,
      388,
      883,
      892
    ],
    "text": "VOR gain adaptation The VOR gain isaratio that compares the eye rotation rate ( numerator) to counter the rotation and translation rate of the head ( denominator). Because head motion has six DOFs, itisappropriate to break the gain into six components. In the case of head pitch and yaw, the VOR gain is close to 1. 0. For example, if you yaw your head to the left at $1 0 ~ \\textdegree$ per second, then your eyes yaw at $1 0 \\textdegree$ per second in the opposite direction. The VOR roll gain is very small because the eyes have a tiny roll range. The VOR translational gain depends on the distance to the features. The VOR comfortably adapts to this problem by changing the gain. Now suppose that you are wearing aVRheadset that may suffer from flaws such asanimperfect optical sys- tem, tracking latency, and incorrectly rendered objects on the screen. In this case, adaptation may occur as the brain attempts to adapt its perception of stationarity to compensate for the flaws. In this case, your visual system could convince your brain that the headset is functioning correctly, and then your perception of stationarity in the real world would become distorted until you readapt. For example, after a flawed VR experience, you might yaw your head in the real world and have the sensation that truly stationary objects are sliding back and forth! 2 Display scanout cameras have either a rolling or global shutter based on whether the sensing elements are scanned line-by-line orinparallel. Displays work the same way, but whereas cameras are an input device, displays are the output analog. Most displays today have a rolling scanout ( called raster scan), rather than global scanout. This implies that the pixels are updated line by line This procedure isanartifact of old TV sets and monitors, which each had a cathode ray tube ( CRT) with phosphor elements on the screen. An electron beam was bent by electromagnets so that it would repeatedly strike and refresh the glowing phosphors. Due to the slow charge and response time of photoreceptors, wedonot perceive the scanout pattern during normal use. However, when ur eyes, features in the scene, or both are moving, then side effects of the rolling scanout may ecome perceptible.\\",
    "raw_text": "VOR gain adaptation The VOR gain is a ratio that compares the eye rotation rate (numerator) to counter the rotation and translation rate of the head (denominator). Because head motion has six DOFs, it is appropriate to break the gain into six components. In the case of head pitch and yaw, the VOR gain is close to 1.0. For example, if you yaw your head to the left at $1 0 ~ \\textdegree$ per second, then your eyes yaw at $1 0 \\textdegree$ per second in the opposite direction. The VOR roll gain is very small because the eyes have a tiny roll range. The VOR translational gain depends on the distance to the features.The VOR comfortably adapts to this problem by changing the gain. Now suppose that you are wearing a VR headset that may suffer from flaws such as an imperfect optical sys- tem, tracking latency, and incorrectly rendered objects on the screen. In this case, adaptation may occur as the brain attempts to adapt its perception of stationarity to compensate for the flaws. In this case, your visual system could convince your brain that the headset is functioning correctly, and then your perception of stationarity in the real world would become distorted until you readapt. For example, after a flawed VR experience, you might yaw your head in the real world and have the sensation that truly stationary objects are sliding back and forth!2 Display scanout cameras have either a rolling or global shutter based on whether the sensing elements are scanned line-by-line or in parallel. Displays work the same way, but whereas cameras are an input device, displays are the output analog. Most displays today have a rolling scanout (called raster scan), rather than global scanout. This implies that the pixels are updated line by line This procedure is an artifact of old TV sets and monitors, which each had a cathode ray tube (CRT) with phosphor elements on the screen. An electron beam was bent by electromagnets so that it would repeatedly strike and refresh the glowing phosphors. Due to the slow charge and response time of photoreceptors, we do not perceive the scanout pattern during normal use. However, when ur eyes, features in the scene, or both are moving, then side effects of the rolling scanout may ecome perceptible.\\",
    "section_hierarchy": "5. Vergence -Stereopsis refers to the case in which both eyes are fixated on the same object, resulting inasingle perceived image. Two kinds of vergence motions occur to align the eyes with an object. If the object is closer than a previous fixation, then a convergence motion occurs. This means that the eyes are rotating so that the pupils are becoming closer. If the object is further, then divergence motion occurs, which causes the pupils to move further apart. The eye orientations resulting from vergence motions provide important information about the distance of objects. > 6. 3. 2 Issues inVRapplications due to eye movement > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 2,
    "char_count": 2241,
    "word_count": 378,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.376728",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303d8"
  },
  {
    "global_chunk_id": "d28c7e34-40a3-4570-a634-ffffd28c4562",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_47",
    "chunk_index": 47,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 8,
    "bbox": [
      116,
      130,
      325,
      147
    ],
    "text": "Academic Year: 2025-26",
    "raw_text": "Academic Year: 2025-26",
    "section_hierarchy": "6. 3. 2 Issues inVRapplications due to eye movement > St. Francis Institute of Technology Department of information Technology > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 22,
    "word_count": 3,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.376728",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303d9"
  },
  {
    "global_chunk_id": "c8c17d2d-ee84-409e-a209-7e6d0579b6f9",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_48",
    "chunk_index": 48,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 8,
    "bbox": [
      393,
      128,
      633,
      147
    ],
    "text": "Class-BE( INFT) Div- A &B",
    "raw_text": "Class-BE(INFT) Div- A &B",
    "section_hierarchy": "6. 3. 2 Issues inVRapplications due to eye movement > St. Francis Institute of Technology Department of information Technology > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 25,
    "word_count": 5,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.376728",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303da"
  },
  {
    "global_chunk_id": "ae3e6666-fd94-4d98-87be-637580e5545f",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_49",
    "chunk_index": 49,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 8,
    "bbox": [
      114,
      227,
      883,
      327
    ],
    "text": "Figure 5. 26 shows how a rectangle would distort under cases of smooth pursuit and VOR. O possibility istofix this by rendering a distorted image that will be corrected by the distortion dtothe line-by-line scanout [ 217] ( this was later suggested in [ 1]). Constructing these imag requires precise calculations of the scanout timings. Yet another problem with displays is that the pixels could take so long to switch ( up to $2 0 \\mathrm { m s }$) that sharp edges appear tobeblurred.",
    "raw_text": "Figure 5.26 shows how a rectangle would distort under cases of smooth pursuit and VOR. O possibility is to fix this by rendering a distorted image that will be corrected by the distortion d to the line-by-line scanout [217] (this was later suggested in [1]). Constructing these imag requires precise calculations of the scanout timings. Yet another problem with displays is that the pixels could take so long to switch (up to $2 0 \\mathrm { m s }$ ) that sharp edges appear to be blurred.",
    "section_hierarchy": "6. 3. 2 Issues inVRapplications due to eye movement > St. Francis Institute of Technology Department of information Technology > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 486,
    "word_count": 85,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.383235",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303db"
  },
  {
    "global_chunk_id": "a6a46d2e-da8e-4489-8e39-4ff4aa26592f",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_50",
    "chunk_index": 50,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 8,
    "bbox": [
      114,
      532,
      883,
      731
    ],
    "text": "Retinal image slip Recall that eye movements contribute both to maintaining a target inafixed location on the retina ( smooth pursuit, VOR) and also to changing its location slightly to reduce perceptual fading ( microsaccades). During ordinary activities ( not VR), the eyes move and the image ofafeature may move slightly on the retina due to motions and optical distortions. This is called retinal image slip. Once aVRheadset is used, the motions of image features on the retina might not match what would happen in the real world. This is due tomany factors already mentioned, such as optical distortions, tracking latency, and display scanout. Thus, the retinal image slip due toVRartifacts does not match the retinal image slip encountered in the real world. The consequences of this have barely been identified, much less characterized scientifically. They are likely to contribute to fatigue, and possibly VR sickness.",
    "raw_text": "Retinal image slip Recall that eye movements contribute both to maintaining a target in a fixed location on the retina (smooth pursuit, VOR) and also to changing its location slightly to reduce perceptual fading (microsaccades). During ordinary activities (not VR), the eyes move and the image of a feature may move slightly on the retina due to motions and optical distortions. This is called retinal image slip. Once a VR headset is used, the motions of image features on the retina might not match what would happen in the real world. This is due tomany factors already mentioned, such as optical distortions, tracking latency, and display scanout. Thus, the retinal image slip due to VR artifacts does not match the retinal image slip encountered in the real world. The consequences of this have barely been identified, much less characterized scientifically. They are likely to contribute to fatigue, and possibly VR sickness.",
    "section_hierarchy": "6. 3. 2 Issues inVRapplications due to eye movement > St. Francis Institute of Technology Department of information Technology > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 926,
    "word_count": 146,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.385014",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303dc"
  },
  {
    "global_chunk_id": "4cef8a16-208e-4cb2-8f28-64ee1429a6ed",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_51",
    "chunk_index": 51,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 8,
    "bbox": [
      114,
      756,
      883,
      895
    ],
    "text": "Vergence-accommodation mismatch accommodation is the process of changing the eye lens’ optical power so that close objects can be brought into focus. This normally occurs with both eyes fixated on the same object, resulting inastereoscopic view that is brought into focus. In the real world, the vergence motion of the eyes and the accommodation of the lens are tightly coupled. For example, if you place your finger $1 0 \\mathrm { c m }$ in front of your face, then your eyes will try to increase he lens power while the eyes are strongly converging. Ifalens is placed atadistance of its focal ength from a screen, then with normal eyes it will always beinfocus while the eye is relaxed.",
    "raw_text": "Vergence-accommodation mismatch accommodation is the process of changing the eye lens’ optical power so that close objects can be brought into focus. This normally occurs with both eyes fixated on the same object, resulting in a stereoscopic view that is brought into focus. In the real world, the vergence motion of the eyes and the accommodation of the lens are tightly coupled. For example, if you place your finger $1 0 \\mathrm { c m }$ in front of your face, then your eyes will try to increase he lens power while the eyes are strongly converging. If a lens is placed at a distance of its focal ength from a screen, then with normal eyes it will always be in focus while the eye is relaxed.",
    "section_hierarchy": "6. 3. 2 Issues inVRapplications due to eye movement > St. Francis Institute of Technology Department of information Technology > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 688,
    "word_count": 119,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.385791",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303dd"
  },
  {
    "global_chunk_id": "512484fd-4328-4426-8c5c-37df3acb3fe2",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_52",
    "chunk_index": 52,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 9,
    "bbox": [
      116,
      130,
      325,
      147
    ],
    "text": "Academic Year: 2025-26",
    "raw_text": "Academic Year: 2025-26",
    "section_hierarchy": "St. Francis Institute of Technology Department of information Technology > St. Francis Institute of Technology Department of information Technology > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 22,
    "word_count": 3,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.385791",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303de"
  },
  {
    "global_chunk_id": "f0a51d52-77ca-4990-9c15-4af62418dad7",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_53",
    "chunk_index": 53,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 9,
    "bbox": [
      393,
      128,
      633,
      147
    ],
    "text": "Class-BE( INFT) Div- A &B",
    "raw_text": "Class-BE(INFT) Div- A &B",
    "section_hierarchy": "St. Francis Institute of Technology Department of information Technology > St. Francis Institute of Technology Department of information Technology > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 25,
    "word_count": 5,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.385791",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303df"
  },
  {
    "global_chunk_id": "be464401-67b9-473c-8c9e-580c0aa1f67b",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_54",
    "chunk_index": 54,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 9,
    "bbox": [
      114,
      227,
      883,
      426
    ],
    "text": "What ifanobject is rendered to the screen so that it appears tobeonly $1 0 \\mathrm { { c m } }$ away? In this ca the eyes strongly converge, but they do not need to change the optical power of the eye lens. T eyes may nevertheless try to accommodate, which would have the effect of blurring the perceiv image. The result is called vergence-accommodation mismatch because the stimulus provided byVRis inconsistent with the real world. Even if the eyes become accustomed to the mismatch, the user may feel extra strain or fatigue after prolonged use. The eyes are essentially being trained to allow a new degree of freedom: Separating vergence from accommodation, rather than coupling them. New display technologies may provide some relief from this problem, but they are currently too costly and imprecise. For example, the mismatch can be greatly reduced by using eye tracking to estimate the amount of vergence and then altering the power of the optical system.",
    "raw_text": "What if an object is rendered to the screen so that it appears to be only $1 0 \\mathrm { { c m } }$ away? In this ca the eyes strongly converge, but they do not need to change the optical power of the eye lens. T eyes may nevertheless try to accommodate, which would have the effect of blurring the perceiv image. The result is called vergence-accommodation mismatch because the stimulus provided by VR is inconsistent with the real world. Even if the eyes become accustomed to the mismatch, the user may feel extra strain or fatigue after prolonged use. The eyes are essentially being trained to allow a new degree of freedom: Separating vergence from accommodation, rather than coupling them. New display technologies may provide some relief from this problem, but they are currently too costly and imprecise. For example, the mismatch can be greatly reduced by using eye tracking to estimate the amount of vergence and then altering the power of the optical system.",
    "section_hierarchy": "St. Francis Institute of Technology Department of information Technology > St. Francis Institute of Technology Department of information Technology > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 962,
    "word_count": 162,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.387298",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303e0"
  },
  {
    "global_chunk_id": "543fe6d0-89a1-4f15-a785-6197c0a4a425",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_55",
    "chunk_index": 55,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 9,
    "bbox": [
      114,
      479,
      883,
      696
    ],
    "text": "Visual Neuroscience isabranch of neuroscience that focuses on the visual system of the human body, mainly located in the brain's visual cortex. The main goal of visual neuroscience istounderstand how neural activity results in visual perception, as well as behaviors dependent on vision. In the past, visual neuroscience has focused primarily on how the brain ( and in particular the Visual Cortex) responds to light rays projected from static images and onto the retina. While this provides a reasonable explanation for the visual perception ofastatic image, it does not provide an accurate explanation for how we perceive the world asitreally is, an ever-changing, and ever-moving 3-D environment. The topics summarized below are representative of this area, but far from exhaustive. Tobeless topic specific, one can see this textbook for the computational link between neural activities and visual perception and behavior: \"Understanding vision: theory, models, and data\", published by Oxford university Press 2014.",
    "raw_text": "Visual Neuroscience is a branch of neuroscience that focuses on the visual system of the human body, mainly located in the brain's visual cortex. The main goal of visual neuroscience is to understand how neural activity results in visual perception, as well as behaviors dependent on vision. In the past, visual neuroscience has focused primarily on how the brain (and in particular the Visual Cortex) responds to light rays projected from static images and onto the retina. While this provides a reasonable explanation for the visual perception of a static image, it does not provide an accurate explanation for how we perceive the world as it really is, an ever-changing, and ever-moving 3-D environment. The topics summarized below are representative of this area, but far from exhaustive. To be less topic specific, one can see this textbook for the computational link between neural activities and visual perception and behavior: \"Understanding vision: theory, models, and data\" , published by Oxford University Press 2014.",
    "section_hierarchy": "St. Francis Institute of Technology Department of information Technology > St. Francis Institute of Technology Department of information Technology > 6. 4 Neuroscience of vision",
    "heading_context": "6. 4 Neuroscience of vision",
    "num_merged_blocks": 1,
    "char_count": 1018,
    "word_count": 152,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.388298",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303e1"
  },
  {
    "global_chunk_id": "dfb1444e-ab05-454c-a2c2-558243cceac9",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_56",
    "chunk_index": 56,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 9,
    "bbox": [
      116,
      791,
      883,
      869
    ],
    "text": "Ifadepth cue is derived from the photoreceptors or movements ofasingle eye, then itiscalled a monocular depth cue. If both eyes are required, then itisa stereo depth cue. There are many more monocular depth cues than stereo, which explains why we are able to infer so much depth formation from a single photograph.",
    "raw_text": "If a depth cue is derived from the photoreceptors or movements of a single eye, then it is called a monocular depth cue. If both eyes are required, then it is a stereo depth cue. There are many more monocular depth cues than stereo, which explains why we are able to infer so much depth formation from a single photograph.",
    "section_hierarchy": "6. 4 Neuroscience of vision > 6. 5 Depth and motion perception > Depth Perception",
    "heading_context": "Depth Perception",
    "num_merged_blocks": 1,
    "char_count": 314,
    "word_count": 52,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.388298",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303e2"
  },
  {
    "global_chunk_id": "a69655a9-ed6d-4c3f-b56b-4220c426fa7a",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_57",
    "chunk_index": 57,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 10,
    "bbox": [
      116,
      130,
      325,
      147
    ],
    "text": "Academic Year: 2025-26",
    "raw_text": "Academic Year: 2025-26",
    "section_hierarchy": "6. 5 Depth and motion perception > Depth Perception > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 22,
    "word_count": 3,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.388298",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303e3"
  },
  {
    "global_chunk_id": "33f6b7a0-24d7-4059-a12d-2d5331bf0820",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_58",
    "chunk_index": 58,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 10,
    "bbox": [
      393,
      128,
      633,
      147
    ],
    "text": "Class-BE( INFT) Div- A &B",
    "raw_text": "Class-BE(INFT) Div- A &B",
    "section_hierarchy": "6. 5 Depth and motion perception > Depth Perception > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 25,
    "word_count": 5,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.388298",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303e4"
  },
  {
    "global_chunk_id": "3c635906-0f48-4cc8-8f87-bbe2a1aad477",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_59",
    "chunk_index": 59,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 10,
    "bbox": [
      114,
      247,
      883,
      386
    ],
    "text": "Retinal image size-Many cues result from the geometric distortions caused by perspec projection. For a familiar object, such asahuman, coin, or basketball, we often judge its distance by how “large” is appears to be. The size of the image on the retina is proportional to $1 / z.$, in which zisthe distance from the eye ( or the common convergence point for all projection lines). The same thing happens when taking a picture with a camera: A picture ofabasketball would occupy larger part of the image, covering more pixels, asitbecomes closer to the camera. This cue is called retinal image size.",
    "raw_text": "Retinal image size-Many cues result from the geometric distortions caused by perspec projection . For a familiar object, such as a human, coin, or basketball, we often judge its distance by how “large” is appears to be.The size of the image on the retina is proportional to $1 / z .$ , in which z is the distance from the eye (or the common convergence point for all projection lines). The same thing happens when taking a picture with a camera: A picture of a basketball would occupy larger part of the image, covering more pixels, as it becomes closer to the camera. This cue is called retinal image size.",
    "section_hierarchy": "Depth Perception > St. Francis Institute of Technology Department of information Technology > Monocular Depth Cues-",
    "heading_context": "Monocular Depth Cues-",
    "num_merged_blocks": 1,
    "char_count": 598,
    "word_count": 101,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.389303",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303e5"
  },
  {
    "global_chunk_id": "48aaf77b-c821-4adf-a38f-57d4e2fe33f9",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_60",
    "chunk_index": 60,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 10,
    "bbox": [
      116,
      407,
      883,
      486
    ],
    "text": "Two important factors exist. First, the viewer must be familiar with the object to the point of comfortably knowing its true size. For familiar objects, such as people or cars, our brains performance size constancy scaling by assuming that the distance, rather than the size, of the person is changing if they come closer.",
    "raw_text": "Two important factors exist. First, the viewer must be familiar with the object to the point of comfortably knowing its true size. For familiar objects, such as people or cars, our brains performance size constancy scaling by assuming that the distance, rather than the size, of the person is changing if they come closer.",
    "section_hierarchy": "Depth Perception > St. Francis Institute of Technology Department of information Technology > Monocular Depth Cues-",
    "heading_context": "Monocular Depth Cues-",
    "num_merged_blocks": 1,
    "char_count": 322,
    "word_count": 54,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.390306",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303e6"
  },
  {
    "global_chunk_id": "5d6e8db2-9222-40c6-ad05-edffa0e2380b",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_61",
    "chunk_index": 61,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 10,
    "bbox": [
      116,
      511,
      883,
      608
    ],
    "text": "Size constancy falls of the general heading of subjective constancy, which appears through many aspects of perception, including shape, size, and color. The second factor is that, the object must be appear naturally so that it does not conflict with other depth cues. If there is significant uncertainty about the size ofanobject, then knowledge of its distance should contribute to estimating its size.",
    "raw_text": "Size constancy falls of the general heading of subjective constancy, which appears through many aspects of perception, including shape, size, and color. The second factor is that, the object must be appear naturally so that it does not conflict with other depth cues. If there is significant uncertainty about the size of an object, then knowledge of its distance should contribute to estimating its size.",
    "section_hierarchy": "Depth Perception > St. Francis Institute of Technology Department of information Technology > Monocular Depth Cues-",
    "heading_context": "Monocular Depth Cues-",
    "num_merged_blocks": 1,
    "char_count": 403,
    "word_count": 63,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.390306",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303e7"
  },
  {
    "global_chunk_id": "7bd41ef9-c75c-4ebb-805d-1a905efcfb35",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_62",
    "chunk_index": 62,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 10,
    "bbox": [
      116,
      631,
      883,
      729
    ],
    "text": "This falls under size perception, which is closely coupled to depth perception. Cues for each influence the other, inaway discussed in Section 6. 4. One controversial theory is that our perceived visual angle differs from the actual visual angle. The visual angle is proportional to the retinal image size. This theory is used to explain the illusion that the moon appears tobelarger when itisnear the horizon.",
    "raw_text": "This falls under size perception, which is closely coupled to depth perception. Cues for each influence the other, in a way discussed in Section 6.4. One controversial theory is that our perceived visual angle differs from the actual visual angle. The visual angle is proportional to the retinal image size. This theory is used to explain the illusion that the moon appears to be larger when it is near the horizon.",
    "section_hierarchy": "Depth Perception > St. Francis Institute of Technology Department of information Technology > Monocular Depth Cues-",
    "heading_context": "Monocular Depth Cues-",
    "num_merged_blocks": 1,
    "char_count": 410,
    "word_count": 66,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.391303",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303e8"
  },
  {
    "global_chunk_id": "0cb03ca7-9f30-44c2-bf1d-f53eabdb288f",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_63",
    "chunk_index": 63,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 10,
    "bbox": [
      114,
      772,
      883,
      869
    ],
    "text": "Suppose that we can see over a long distance without obstructions. Due to perspective projection, the horizon isaline that divides the view in half. The upper half is perceived as the sky, and the lower half is the ground. The distance of objects from the horizon line corresponds directly to their distance due to perspective projection: The closer to the horizon, the further the perceived distance. ize constancy scaling, if available, combines with the height in the visual field.",
    "raw_text": "Suppose that we can see over a long distance without obstructions. Due to perspective projection, the horizon is a line that divides the view in half. The upper half is perceived as the sky, and the lower half is the ground. The distance of objects from the horizon line corresponds directly to their distance due to perspective projection: The closer to the horizon, the further the perceived distance. ize constancy scaling, if available, combines with the height in the visual field.",
    "section_hierarchy": "St. Francis Institute of Technology Department of information Technology > Monocular Depth Cues- > Height in the visual field",
    "heading_context": "Height in the visual field",
    "num_merged_blocks": 1,
    "char_count": 484,
    "word_count": 79,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.391303",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303e9"
  },
  {
    "global_chunk_id": "45702677-9a0e-4681-bd60-e5d3955c11a9",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_64",
    "chunk_index": 64,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 11,
    "bbox": [
      116,
      130,
      325,
      147
    ],
    "text": "Academic Year: 2025-26",
    "raw_text": "Academic Year: 2025-26",
    "section_hierarchy": "Monocular Depth Cues- > Height in the visual field > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 22,
    "word_count": 3,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.391303",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303ea"
  },
  {
    "global_chunk_id": "49db413b-eb50-45f8-9571-4e5d97420db5",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_65",
    "chunk_index": 65,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 11,
    "bbox": [
      393,
      128,
      633,
      147
    ],
    "text": "Class-BE( INFT) Div- A &B",
    "raw_text": "Class-BE(INFT) Div- A &B",
    "section_hierarchy": "Monocular Depth Cues- > Height in the visual field > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 25,
    "word_count": 5,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.392304",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303eb"
  },
  {
    "global_chunk_id": "1cbd6b0d-bfe1-4c03-9508-866626ed5035",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_66",
    "chunk_index": 66,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 11,
    "bbox": [
      748,
      131,
      877,
      147
    ],
    "text": "Subject-ARVR",
    "raw_text": "Subject-ARVR",
    "section_hierarchy": "Monocular Depth Cues- > Height in the visual field > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 12,
    "word_count": 1,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.392304",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303ec"
  },
  {
    "global_chunk_id": "727b8215-ade2-4d84-b5a3-ce04dc9dbeea",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_67",
    "chunk_index": 67,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 11,
    "bbox": [
      114,
      227,
      883,
      327
    ],
    "text": "Accommodation-The human eye lens can change its optical power through the process accommodation. For young adults, the amount of change is around 10D ( diopters), but it decrea to less than 1D for adults over 50 years old. The ciliary muscles control the lens and their tensi level is reported to the brain through efference copies of the motor control signal. This is the first depth cue that does not depend on signals generated by the photoreceptors.",
    "raw_text": "Accommodation-The human eye lens can change its optical power through the process accommodation. For young adults, the amount of change is around 10D (diopters), but it decrea to less than 1D for adults over 50 years old. The ciliary muscles control the lens and their tensi level is reported to the brain through efference copies of the motor control signal. This is the first depth cue that does not depend on signals generated by the photoreceptors.",
    "section_hierarchy": "Monocular Depth Cues- > Height in the visual field > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 453,
    "word_count": 77,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.392304",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303ed"
  },
  {
    "global_chunk_id": "e955b1e6-7608-49f9-b815-8475f2172b5d",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_68",
    "chunk_index": 68,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 11,
    "bbox": [
      114,
      328,
      883,
      386
    ],
    "text": "Motion parallax- Up until now, the depth cues have not exploited motions. If you have ever looked out of the side window ofafast-moving vehicle, you might have noticed that the nearby objects race by much faster than further objects.",
    "raw_text": "Motion parallax- Up until now, the depth cues have not exploited motions. If you have ever looked out of the side window of a fast-moving vehicle, you might have noticed that the nearby objects race by much faster than further objects.",
    "section_hierarchy": "Monocular Depth Cues- > Height in the visual field > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 233,
    "word_count": 39,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.393305",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303ee"
  },
  {
    "global_chunk_id": "f2ac64c1-1cf8-42e2-98e0-9a4469fe6726",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_69",
    "chunk_index": 69,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 11,
    "bbox": [
      114,
      410,
      883,
      569
    ],
    "text": "The relative difference in speeds is called parallax and isanimportant depth cue; see Even two images, from varying viewpoints within a short amount of time, provide strong depth information. Imagine trying to simulate a stereo rig of cameras by snapping one photo and quickly moving the camera sideways to snap another. If the rest of the world is stationary, then the result is roughly equivalent to having two side-by-side cameras. Pigeons frequently bob their heads back and forth to obtain stronger depth information than is provided by their pair of eyes. Finally, closely related to motion parallax is optical flow, which isacharacterization of the rates at which features move across the retina.",
    "raw_text": "The relative difference in speeds is called parallax and is an important depth cue; see Even two images, from varying viewpoints within a short amount of time, provide strong depth information. Imagine trying to simulate a stereo rig of cameras by snapping one photo and quickly moving the camera sideways to snap another. If the rest of the world is stationary, then the result is roughly equivalent to having two side-by-side cameras. Pigeons frequently bob their heads back and forth to obtain stronger depth information than is provided by their pair of eyes. Finally, closely related to motion parallax is optical flow, which is a characterization of the rates at which features move across the retina.",
    "section_hierarchy": "Monocular Depth Cues- > Height in the visual field > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 703,
    "word_count": 112,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.394298",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303ef"
  },
  {
    "global_chunk_id": "28538c31-abe0-4c28-89e1-16018f0903b4",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_70",
    "chunk_index": 70,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 11,
    "bbox": [
      114,
      690,
      883,
      890
    ],
    "text": "As you may expect, focusing both eyes on the same object enhances depth perception. Humans perceive a single focused image over a surface in space called the horopter. Similar to the accommodation cue case, motor control of the eye muscles for vergence motions provides information to the brain about the amount of convergence, thereby providing a direct estimate of distance. Each eye provides a different viewpoint, which results in different images on the retina. This phenomenon is called binocular disparity. The viewpoint is shifted to the right or left to provide a lateral offset for each of the eyes. The transform essentially shifts the virtual world to either side. The same shift would happen for a stereo rig of side-by-side cameras in the real world. owever, the binocular disparity for humans is different because the eyes can rotate to converge, n addition to having a lateral offset. Thus, when fixating onanobject, the retinal images between",
    "raw_text": "As you may expect, focusing both eyes on the same object enhances depth perception. Humans perceive a single focused image over a surface in space called the horopter. Similar to the accommodation cue case, motor control of the eye muscles for vergence motions provides information to the brain about the amount of convergence, thereby providing a direct estimate of distance. Each eye provides a different viewpoint, which results in different images on the retina. This phenomenon is called binocular disparity. The viewpoint is shifted to the right or left to provide a lateral offset for each of the eyes. The transform essentially shifts the virtual world to either side. The same shift would happen for a stereo rig of side-by-side cameras in the real world. owever, the binocular disparity for humans is different because the eyes can rotate to converge, n addition to having a lateral offset. Thus, when fixating on an object, the retinal images between",
    "section_hierarchy": "Height in the visual field > St. Francis Institute of Technology Department of information Technology > Stereo Depth Cues----",
    "heading_context": "Stereo Depth Cues----",
    "num_merged_blocks": 1,
    "char_count": 959,
    "word_count": 155,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.395302",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303f0"
  },
  {
    "global_chunk_id": "079ee984-19a8-4adc-ae61-e35a1c43d2bb",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_71",
    "chunk_index": 71,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 12,
    "bbox": [
      116,
      130,
      325,
      147
    ],
    "text": "Academic Year: 2025-26",
    "raw_text": "Academic Year: 2025-26",
    "section_hierarchy": "St. Francis Institute of Technology Department of information Technology > Stereo Depth Cues---- > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 22,
    "word_count": 3,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.395811",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303f1"
  },
  {
    "global_chunk_id": "8eede059-51cc-41a6-b045-fc8c38ce7bb8",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_72",
    "chunk_index": 72,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 12,
    "bbox": [
      393,
      128,
      633,
      147
    ],
    "text": "Class-BE( INFT) Div- A &B",
    "raw_text": "Class-BE(INFT) Div- A &B",
    "section_hierarchy": "St. Francis Institute of Technology Department of information Technology > Stereo Depth Cues---- > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 25,
    "word_count": 5,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.395811",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303f2"
  },
  {
    "global_chunk_id": "a0ea18ae-4388-4482-9430-274b5080669a",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_73",
    "chunk_index": 73,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 12,
    "bbox": [
      114,
      227,
      883,
      306
    ],
    "text": "the left and right eyes may vary only slightly, but this nevertheless provides a powerful cue usbythe brain Furthermore, when converging onanobject at one depth, we perceive double images of objects other depths ( although we usually pay no attention to it).",
    "raw_text": "the left and right eyes may vary only slightly, but this nevertheless provides a powerful cue us by the brain Furthermore, when converging on an object at one depth, we perceive double images of objects other depths (although we usually pay no attention to it).",
    "section_hierarchy": "St. Francis Institute of Technology Department of information Technology > Stereo Depth Cues---- > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 2,
    "char_count": 258,
    "word_count": 42,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.395811",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303f3"
  },
  {
    "global_chunk_id": "29b9c647-d4f0-496a-8538-49db5b67ae7d",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_74",
    "chunk_index": 74,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 12,
    "bbox": [
      114,
      330,
      883,
      488
    ],
    "text": "This double-image effect is called diplopia. You can perceive itbyplacing your finger about $2 0 \\mathrm { c m }$ in front of your face and converging on it. While fixating on your finger, you should perceive double images of other objects around the periphery. You can also stare into the distance while keeping your finger in the same place. You should see a double image of your finger. If you additionally roll your head back and forth, it should appear asifthe left and right versions of your finger are moving up and down with respect to each other. These correspond to dramatic differences in the retinal image, but we are usually not aware of them because we perceive both retinal images asasingle image.",
    "raw_text": "This double-image effect is called diplopia. You can perceive it by placing your finger about $2 0 \\mathrm { c m }$ in front of your face and converging on it. While fixating on your finger, you should perceive double images of other objects around the periphery. You can also stare into the distance while keeping your finger in the same place. You should see a double image of your finger. If you additionally roll your head back and forth, it should appear as if the left and right versions of your finger are moving up and down with respect to each other. These correspond to dramatic differences in the retinal image, but we are usually not aware of them because we perceive both retinal images as a single image.",
    "section_hierarchy": "St. Francis Institute of Technology Department of information Technology > Stereo Depth Cues---- > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 712,
    "word_count": 124,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.396826",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303f4"
  },
  {
    "global_chunk_id": "91e2b42e-5802-466c-a5d5-236d89bba262",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_75",
    "chunk_index": 75,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 12,
    "bbox": [
      116,
      491,
      883,
      609
    ],
    "text": "Motion Perception We rely on our vision to perceive motion for many crucial activities. One use istoseparate a moving figure from a stationary background. For example, a camouflaged animal in the forest might only become noticeable when moving. This is clearly useful whether humans are the hunter or the hunted. Motion also helps people to assess the 3D structure ofanobject. Imagine assessing the value ofapiece of fruit in the market by rotating it around. Another use istovisually guide actions, such as walking down the street or hammering a nail. VR systems have the tall order of replicating these uses inavirtual world in spite of limited technology. Just as important as the perception of motion is the perception of nonmotion, which we called perception of stationarity Stroboscopic apparent motion Nearly everyone on Earth has seen a motion picture, whether through a TV, smartphone, or movie screen. The motions we see are an illusion because a sequence of still pictures is being flashed onto the screen. This phenomenon is called stroboscopic apparent motion; it was discovered and refined across the 19th century. The zoetrope, shown in Figure 6. 15 was developed around 1834. It consists ofarotating drum with slits that allow each frame tobevisible for an instant while the drum rotates.",
    "raw_text": "Motion Perception We rely on our vision to perceive motion for many crucial activities. One use is to separate a moving figure from a stationary background. For example, a camouflaged animal in the forest might only become noticeable when moving. This is clearly useful whether humans are the hunter or the hunted. Motion also helps people to assess the 3D structure of an object. Imagine assessing the value of a piece of fruit in the market by rotating it around. Another use is to visually guide actions, such as walking down the street or hammering a nail. VR systems have the tall order of replicating these uses in a virtual world in spite of limited technology. Just as important as the perception of motion is the perception of nonmotion, which we called perception of stationarity Stroboscopic apparent motion Nearly everyone on Earth has seen a motion picture, whether through a TV, smartphone, or movie screen. The motions we see are an illusion because a sequence of still pictures is being flashed onto the screen. This phenomenon is called stroboscopic apparent motion; it was discovered and refined across the 19th century . The zoetrope, shown in Figure 6.15 was developed around 1834. It consists of a rotating drum with slits that allow each frame to be visible for an instant while the drum rotates.",
    "section_hierarchy": "St. Francis Institute of Technology Department of information Technology > Stereo Depth Cues---- > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 2,
    "char_count": 1304,
    "word_count": 209,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.397391",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303f5"
  },
  {
    "global_chunk_id": "8ce6903b-9d78-4bcb-b135-eed74493f982",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_76",
    "chunk_index": 76,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 13,
    "bbox": [
      116,
      130,
      325,
      147
    ],
    "text": "Academic Year: 2025-26",
    "raw_text": "Academic Year: 2025-26",
    "section_hierarchy": "6. 6 Frame rates and display > 6. 1 Frame Rates > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 22,
    "word_count": 3,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.398404",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303f6"
  },
  {
    "global_chunk_id": "6ffbf2fd-7dfa-4b84-8c4a-25c0c153255b",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_77",
    "chunk_index": 77,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 13,
    "bbox": [
      393,
      128,
      633,
      147
    ],
    "text": "Class-BE( INFT) Div- A &B",
    "raw_text": "Class-BE(INFT) Div- A &B",
    "section_hierarchy": "6. 6 Frame rates and display > 6. 1 Frame Rates > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 25,
    "word_count": 5,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.398404",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303f7"
  },
  {
    "global_chunk_id": "ce7ad825-4f92-4913-a5bd-ef26bf9316cb",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_78",
    "chunk_index": 78,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 13,
    "bbox": [
      117,
      227,
      643,
      246
    ],
    "text": "How many frames per second are appropriate for a motion picture?",
    "raw_text": "How many frames per second are appropriate for a motion picture?",
    "section_hierarchy": "6. 6 Frame rates and display > 6. 1 Frame Rates > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 64,
    "word_count": 11,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.398404",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303f8"
  },
  {
    "global_chunk_id": "6a5f9030-0e19-44a8-a0af-a197c33a9857",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_79",
    "chunk_index": 79,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 13,
    "bbox": [
      114,
      247,
      883,
      546
    ],
    "text": "The answer depends on the intended use. Stroboscopic apparent motion begins at 2 FPS. Imagi watching a security video at this rate. Itiseasy to distinguish individual frames, but the motion a person would also be perceived. Once 10 FPS is reached, the motion is obviously more smooth and we start to lose the ability to distinguish individual frames. Early silent films ranged from 16 to 24 FPS. The frame rates were often fluctuating and were played atafaster speed than they were filmed. Once sound was added to film, incorrect speeds and fluctuations in the speed were no longer tolerated because both sound and video needed tobesynchronized. This motivated playback at the fixed rate of 24 FPS, which is still used today by the movie industry. Personal video cameras remained at 16 or 18 FPS into the 1970s. The famous Zapruder film of the Kennedy assassination in 1963 was taken at 18. 3 FPS. Although 24 FPS may be enough to perceive motions smoothly, a large part of cinematography is devoted to ensuring that motions are not so fast that jumps are visible due to the low frame rate. Video see-through displays are one of the most popular and widely used display technologies when it comes to virtual reality. It generally takes video images from one or two cameras and superimposes it with digitally generated imagery. The combined image then becomes the display that the user views.",
    "raw_text": "The answer depends on the intended use. Stroboscopic apparent motion begins at 2 FPS. Imagi watching a security video at this rate. It is easy to distinguish individual frames, but the motion a person would also be perceived. Once 10 FPS is reached, the motion is obviously more smooth and we start to lose the ability to distinguish individual frames. Early silent films ranged from 16 to 24 FPS. The frame rates were often fluctuating and were played at a faster speed than they were filmed. Once sound was added to film, incorrect speeds and fluctuations in the speed were no longer tolerated because both sound and video needed to be synchronized. This motivated playback at the fixed rate of 24 FPS, which is still used today by the movie industry. Personal video cameras remained at 16 or 18 FPS into the 1970s. The famous Zapruder film of the Kennedy assassination in 1963 was taken at 18.3 FPS. Although 24 FPS may be enough to perceive motions smoothly, a large part of cinematography is devoted to ensuring that motions are not so fast that jumps are visible due to the low frame rate. Video see-through displays are one of the most popular and widely used display technologies when it comes to virtual reality. It generally takes video images from one or two cameras and superimposes it with digitally generated imagery. The combined image then becomes the display that the user views.",
    "section_hierarchy": "6. 6 Frame rates and display > 6. 1 Frame Rates > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 1391,
    "word_count": 236,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.400403",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303f9"
  },
  {
    "global_chunk_id": "fb7d5549-2fae-4016-bf4e-1a433b1c5550",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_80",
    "chunk_index": 80,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 13,
    "bbox": [
      116,
      652,
      883,
      710
    ],
    "text": "This section explains how the orientation ofarigid body is estimated using an inertial measurement unit ( IMU). The main application is determining the viewpoint orientation, while the user is wearing aVRheadset.",
    "raw_text": "This section explains how the orientation of a rigid body is estimated using an inertial measurement unit (IMU). The main application is determining the viewpoint orientation,while the user is wearing a VR headset.",
    "section_hierarchy": "St. Francis Institute of Technology Department of information Technology > 6. 7 Orientation tracking, tilt and yaw drift correction > 6. 7. 1 Tracking 2D Orientation",
    "heading_context": "6. 7. 1 Tracking 2D Orientation",
    "num_merged_blocks": 1,
    "char_count": 212,
    "word_count": 31,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.400403",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303fa"
  },
  {
    "global_chunk_id": "1493ec0e-e5ff-4a04-becc-8ac57e8d167e",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_81",
    "chunk_index": 81,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 13,
    "bbox": [
      114,
      713,
      883,
      811
    ],
    "text": "Another application is estimating the orientation ofahand-held controller. For example, suppose we would like to make a laser pointer that works in the virtual world, based onadirection indicated by the user. The location ofabright red dot in the scene would be determined by the estimated orientation ofacontroller. More generally, the orientation of any human body part or moving object in the physical world can be determined ifithas an attached IMU.",
    "raw_text": "Another application is estimating the orientation of a hand-held controller. For example, suppose we would like to make a laser pointer that works in the virtual world, based on a direction indicated by the user. The location of a bright red dot in the scene would be determined by the estimated orientation of a controller. More generally, the orientation of any human body part or moving object in the physical world can be determined if it has an attached IMU.",
    "section_hierarchy": "St. Francis Institute of Technology Department of information Technology > 6. 7 Orientation tracking, tilt and yaw drift correction > 6. 7. 1 Tracking 2D Orientation",
    "heading_context": "6. 7. 1 Tracking 2D Orientation",
    "num_merged_blocks": 1,
    "char_count": 453,
    "word_count": 70,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.401054",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303fb"
  },
  {
    "global_chunk_id": "96f50cf6-34ed-410b-a237-21627a5d9c91",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_82",
    "chunk_index": 82,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 13,
    "bbox": [
      112,
      814,
      883,
      892
    ],
    "text": "Four general problems must be solved to make an effective tracking system, even for this simple case: Calibration: Ifabetter sensor is available, then the two can be closely paired so that the outputs f the worse sensor are transformed to behave as closely to the better sensor as possible.",
    "raw_text": "Four general problems must be solved to make an effective tracking system, even for this simple case: Calibration: If a better sensor is available, then the two can be closely paired so that the outputs f the worse sensor are transformed to behave as closely to the better sensor as possible.",
    "section_hierarchy": "St. Francis Institute of Technology Department of information Technology > 6. 7 Orientation tracking, tilt and yaw drift correction > 6. 7. 1 Tracking 2D Orientation",
    "heading_context": "6. 7. 1 Tracking 2D Orientation",
    "num_merged_blocks": 2,
    "char_count": 290,
    "word_count": 49,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.401054",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303fc"
  },
  {
    "global_chunk_id": "8811ee39-ff83-41d5-a142-9133ded1320b",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_83",
    "chunk_index": 83,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 14,
    "bbox": [
      116,
      130,
      325,
      147
    ],
    "text": "Academic Year: 2025-26",
    "raw_text": "Academic Year: 2025-26",
    "section_hierarchy": "6. 7 Orientation tracking, tilt and yaw drift correction > 6. 7. 1 Tracking 2D Orientation > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 22,
    "word_count": 3,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.402069",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303fd"
  },
  {
    "global_chunk_id": "569141e8-9487-49c4-86a8-51507b562a9b",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_84",
    "chunk_index": 84,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 14,
    "bbox": [
      393,
      128,
      633,
      147
    ],
    "text": "Class-BE( INFT) Div- A &B",
    "raw_text": "Class-BE(INFT) Div- A &B",
    "section_hierarchy": "6. 7 Orientation tracking, tilt and yaw drift correction > 6. 7. 1 Tracking 2D Orientation > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 25,
    "word_count": 5,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.402069",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303fe"
  },
  {
    "global_chunk_id": "c3be8262-147d-4aed-a82c-516726938591",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_85",
    "chunk_index": 85,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 14,
    "bbox": [
      114,
      497,
      883,
      695
    ],
    "text": "Calibration-The sensor outputs are distorted due to calibration issues. In the one-dimensional angular velocity case, there were only two parameters, for scale and offset. In the 3D setting, this would naturally extend to 3 scale and 3 offset parameters; however, the situation is worse because there may also be errors due to non-orthogonality of the MEMS elements. All of these can be accounted for by 12 parameters arranged inahomogeneous transform matrix: There are 12 and not 6 DOFs because the upper left, 3-by-3, matrix is not constrained tobea rotation matrix. The j, $\\mathbf { k }$, and ℓ parameters correspond to offset, whereas all others handle scale and non-orthogonality Integration-Now consider the problem converting the sequence of gyroscope outputs into an estimate of the 3D orientation.",
    "raw_text": "Calibration - The sensor outputs are distorted due to calibration issues. In the one-dimensional angular velocity case, there were only two parameters, for scale and offset. In the 3D setting, this would naturally extend to 3 scale and 3 offset parameters; however, the situation is worse because there may also be errors due to non-orthogonality of the MEMS elements. All of these can be accounted for by 12 parameters arranged in a homogeneous transform matrix: There are 12 and not 6 DOFs because the upper left, 3-by-3, matrix is not constrained to be a rotation matrix. The j, $\\mathbf { k }$ , and ℓ parameters correspond to offset, whereas all others handle scale and non-orthogonality Integration - Now consider the problem converting the sequence of gyroscope outputs into an estimate of the 3D orientation.",
    "section_hierarchy": "3. Registration: The initial orientation must somehow be determined, either byanadditional sensor, oraclever default assumption. > 4. Drift error: As the error grows over time, other sensors are needed to directly estimate it and compensate for it. The process of combining information from multiple sensor readings is often called sensor fusion or filtering > 6. 7. 2 Tracking 3D Orientation",
    "heading_context": "6. 7. 2 Tracking 3D Orientation",
    "num_merged_blocks": 3,
    "char_count": 807,
    "word_count": 126,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.404068",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b1303ff"
  },
  {
    "global_chunk_id": "67cb6d81-a217-4947-aa84-23908e28512c",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_86",
    "chunk_index": 86,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 14,
    "bbox": [
      114,
      696,
      883,
      867
    ],
    "text": "At each stage $\\mathrm { k \\Omega }$ a vector $\\boldsymbol { \\omega } \\mathrm { [ k] } = \\mathrm { ( \\hat { \\omega }) ( \\omega \\mathrm { x [ k], \\omega \\mathrm { \\hat { y } [ k], \\omega \\mathrm { \\hat { z } [ k]) } } } }$ arrives from the sensor. In Section 9. 1, the sensor output $\\mathrm { { \\hat { \\omega } } _ { \\mathrm { { G } } } [ k] }$ was converted toachange $\\Delta \\Theta [ \\mathrm { k }]$ in orientation. For the 3D case, the change in orientation is expressed asaquaternion Registration-The registration problem for the yaw component is the same. The forward direction may be chosen from the initial orientation of the rigid body oritcould be determined with respect toafixed direction in the world. The pitch and roll components should be determined so that they align with gravity. The virtual world should not appear tobetilted with respect to the real world less that is the desired effect, which is rarely the case).",
    "raw_text": "At each stage $\\mathrm { k \\Omega }$ a vector $\\boldsymbol { \\omega } \\mathrm { [ k ] } = \\mathrm { ( \\hat { \\omega } ) ( \\omega \\mathrm { x [ k ] , \\omega \\mathrm { \\hat { y } [ k ] , \\omega \\mathrm { \\hat { z } [ k ] ) } } } }$ arrives from the sensor. In Section 9.1, the sensor output $\\mathrm { { \\hat { \\omega } } _ { \\mathrm { { G } } } [ k ] }$ was converted to a change $\\Delta \\Theta [ \\mathrm { k } ]$ in orientation. For the 3D case, the change in orientation is expressed as a quaternion Registration - The registration problem for the yaw component is the same.The forward direction may be chosen from the initial orientation of the rigid body or it could be determined with respect to a fixed direction in the world. The pitch and roll components should be determined so that they align with gravity. The virtual world should not appear to be tilted with respect to the real world less that is the desired effect, which is rarely the case).",
    "section_hierarchy": "3. Registration: The initial orientation must somehow be determined, either byanadditional sensor, oraclever default assumption. > 4. Drift error: As the error grows over time, other sensors are needed to directly estimate it and compensate for it. The process of combining information from multiple sensor readings is often called sensor fusion or filtering > 6. 7. 2 Tracking 3D Orientation",
    "heading_context": "6. 7. 2 Tracking 3D Orientation",
    "num_merged_blocks": 2,
    "char_count": 935,
    "word_count": 182,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.405069",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b130400"
  },
  {
    "global_chunk_id": "34a5a5d3-1044-47ca-bf7c-dc4ca33e41c4",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_87",
    "chunk_index": 87,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 15,
    "bbox": [
      116,
      130,
      325,
      147
    ],
    "text": "Academic Year: 2025-26",
    "raw_text": "Academic Year: 2025-26",
    "section_hierarchy": "4. Drift error: As the error grows over time, other sensors are needed to directly estimate it and compensate for it. The process of combining information from multiple sensor readings is often called sensor fusion or filtering > 6. 7. 2 Tracking 3D Orientation > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 22,
    "word_count": 3,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.405069",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b130401"
  },
  {
    "global_chunk_id": "6c58aa59-fb92-4e4a-9332-bb6bfd90aa2b",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_88",
    "chunk_index": 88,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 15,
    "bbox": [
      393,
      128,
      633,
      147
    ],
    "text": "Class-BE( INFT) Div- A &B",
    "raw_text": "Class-BE(INFT) Div- A &B",
    "section_hierarchy": "4. Drift error: As the error grows over time, other sensors are needed to directly estimate it and compensate for it. The process of combining information from multiple sensor readings is often called sensor fusion or filtering > 6. 7. 2 Tracking 3D Orientation > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 25,
    "word_count": 5,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.406065",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b130402"
  },
  {
    "global_chunk_id": "ed1f8bcd-80a6-48bf-9f33-fb32e804b213",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_89",
    "chunk_index": 89,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 15,
    "bbox": [
      112,
      227,
      885,
      430
    ],
    "text": "Tilt correction -The drift error $\\mathrm { d ( t) }$ in was a single angle, which could be positive or negati If added to the estimate $\\cdot \\Theta ( \\mathfrak { t })$, the true orientation $\\Theta ( \\mathfrak { t })$ would be obtained. Itissimilar for t 3D case, but with quaternion algebra. The 3D drift error is expressed as $\\mathsf { d } ( \\mathsf { t }) = \\mathsf { q } ( \\mathsf { t })$ $\\hat { \\mathsf { q } } ^ { \\star } - 1$ ( t), Yaw correction -The remaining drift error component is detected by a \n“compass”, which outputs a vector that lies in the world xz plane and always points “north”. Suppose this is ${ \\bf { \\hat { n } } } = \\left( { 0, 0, - 1 } \\right)$. Once again, the sensor output occurs in the coordinate frame of the body, and needs tobetransformed by \n$\\mathrm { \\hat { \\varphi } q [ k] }$. The difference between $\\scriptstyle \\mathbf { \\hat { n } }$ and the $- \\mathbf { Z }$ axis is the resulting yaw drift error. \n6. 8 Tracking with camera",
    "raw_text": "Tilt correction -The drift error $\\mathrm { d ( t ) }$ in was a single angle, which could be positive or negati If added to the estimate $\\cdot \\Theta ( \\mathfrak { t } )$ , the true orientation $\\Theta ( \\mathfrak { t } )$ would be obtained. It is similar for t 3D case, but with quaternion algebra. The 3D drift error is expressed as $\\mathsf { d } ( \\mathsf { t } ) = \\mathsf { q } ( \\mathsf { t } )$ $\\hat { \\mathsf { q } } ^ { \\star } - 1$ (t), Yaw correction -The remaining drift error component is detected by a   \n“compass”, which outputs a vector that lies in the world xz plane and always points “north”. Suppose this is ${ \\bf { \\hat { n } } } = \\left( { 0 , 0 , - 1 } \\right)$ . Once again, the sensor output occurs in the coordinate frame of the body, and needs to be transformed by   \n$\\mathrm { \\hat { \\varphi } q [ k ] }$ . The difference between $\\scriptstyle \\mathbf { \\hat { n } }$ and the $- \\mathbf { Z }$ axis is the resulting yaw drift error.   \n6.8 Tracking with camera",
    "section_hierarchy": "4. Drift error: As the error grows over time, other sensors are needed to directly estimate it and compensate for it. The process of combining information from multiple sensor readings is often called sensor fusion or filtering > 6. 7. 2 Tracking 3D Orientation > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 974,
    "word_count": 201,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.406065",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b130403"
  },
  {
    "global_chunk_id": "0a7aece8-4e96-46ad-9413-e4032236523f",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_90",
    "chunk_index": 90,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 15,
    "bbox": [
      114,
      494,
      883,
      613
    ],
    "text": "Six-DOF tracking enables eye from tobefully derived from sensor data, rather than inventing positions from a plausible head model. By estimating the position, the powerful depth cue of parallax becomes much stronger as the user moves her head from side to side. She could even approach a small object and look atitfrom any viewpoint, such as from above, below, or the sides. The methods in this section are also useful for tracking hands in space or objects that are manipulated during aVRexperience.",
    "raw_text": "Six-DOF tracking enables eye from to be fully derived from sensor data, rather than inventing positions from a plausible head model. By estimating the position, the powerful depth cue of parallax becomes much stronger as the user moves her head from side to side. She could even approach a small object and look at it from any viewpoint, such as from above, below, or the sides. The methods in this section are also useful for tracking hands in space or objects that are manipulated during a VR experience.",
    "section_hierarchy": "6. 7. 2 Tracking 3D Orientation > St. Francis Institute of Technology Department of information Technology > 6. 8. 1 Tracking Position and Orientation",
    "heading_context": "6. 8. 1 Tracking Position and Orientation",
    "num_merged_blocks": 1,
    "char_count": 500,
    "word_count": 82,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.407578",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b130404"
  },
  {
    "global_chunk_id": "1c1dedf8-74ec-4c4d-bc10-a06ef1858141",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_91",
    "chunk_index": 91,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 15,
    "bbox": [
      114,
      636,
      883,
      897
    ],
    "text": "Make your own waves The IMU-based approach to tracking was passive in the sense that it relied on sources of information that already exist in the environment. Instead, an active approach can be taken by transmitting waves into the environment. Since humans operate in the same environment, waves that are perceptible, such as light and sound, are not preferred. Instead, common energy sources in active tracking systems include infrared, ultrasound, and electromagnetic fields. Consider transmitting an ultrasound pulse ( above 20, $0 0 0 ~ \\mathrm { H z })$ from a speaker and using a microphone to listen for its arrival. This isanexample ofanemitterdetector pair: The speaker is the emitter, and the microphone is the detector. If time measurement is synchronized between source and destination, then the time of arrival ( TOA or time of flight) can be calculated. This is the time that it took for the pulse to travel the distance d between the emitter and detector. Based on the known propagation speed in the medium $3 3 0 ~ \\mathrm { m / s }$ for ultrasound), the distance $\\mathrm { \\hat { \\Omega } d }$ is stimated. One frustrating limitation of ultrasound systems is reverberation between surfaces, ausing the pulse tobereceived multiple times at each detector.",
    "raw_text": "Make your own waves The IMU-based approach to tracking was passive in the sense that it relied on sources of information that already exist in the environment. Instead, an active approach can be taken by transmitting waves into the environment. Since humans operate in the same environment, waves that are perceptible, such as light and sound, are not preferred. Instead, common energy sources in active tracking systems include infrared, ultrasound, and electromagnetic fields. Consider transmitting an ultrasound pulse (above 20, $0 0 0 ~ \\mathrm { H z } )$ from a speaker and using a microphone to listen for its arrival. This is an example of an emitterdetector pair: The speaker is the emitter, and the microphone is the detector. If time measurement is synchronized between source and destination, then the time of arrival (TOA or time of flight) can be calculated. This is the time that it took for the pulse to travel the distance d between the emitter and detector. Based on the known propagation speed in the medium $3 3 0 ~ \\mathrm { m / s }$ for ultrasound), the distance $\\mathrm { \\hat { \\Omega } d }$ is stimated. One frustrating limitation of ultrasound systems is reverberation between surfaces, ausing the pulse to be received multiple times at each detector.",
    "section_hierarchy": "6. 7. 2 Tracking 3D Orientation > St. Francis Institute of Technology Department of information Technology > 6. 8. 1 Tracking Position and Orientation",
    "heading_context": "6. 8. 1 Tracking Position and Orientation",
    "num_merged_blocks": 1,
    "char_count": 1272,
    "word_count": 212,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.408592",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b130405"
  },
  {
    "global_chunk_id": "d752e08b-208f-4d0c-8f1b-2d3e0d9c0c50",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_92",
    "chunk_index": 92,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 16,
    "bbox": [
      116,
      130,
      325,
      147
    ],
    "text": "Academic Year: 2025-26",
    "raw_text": "Academic Year: 2025-26",
    "section_hierarchy": "St. Francis Institute of Technology Department of information Technology > 6. 8. 1 Tracking Position and Orientation > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 22,
    "word_count": 3,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.408592",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b130406"
  },
  {
    "global_chunk_id": "1a0fa8b0-2e39-40f6-9302-a4ba0c195d03",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_93",
    "chunk_index": 93,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 16,
    "bbox": [
      393,
      128,
      633,
      147
    ],
    "text": "Class-BE( INFT) Div- A &B",
    "raw_text": "Class-BE(INFT) Div- A &B",
    "section_hierarchy": "St. Francis Institute of Technology Department of information Technology > 6. 8. 1 Tracking Position and Orientation > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 25,
    "word_count": 5,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.408592",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b130407"
  },
  {
    "global_chunk_id": "24b835c0-6440-461d-95c4-2ec912e9288f",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_94",
    "chunk_index": 94,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 16,
    "bbox": [
      114,
      227,
      883,
      366
    ],
    "text": "The power of visibility The most powerful paradigm for 6-DOF tracking is visibility. T idea istoidentify special parts of the physical world called features and calculate their positio along a line-of-sight ray toaknown location. Figure 9. 11 shows an example inspired byacame but other hardware could be used. One crucial aspect for tracking is distinguishability. If all features appear tobethe same, then it may become difficult to determine and maintain “which is which” during the tracking process. Each feature should be assigned a unique label that is invariant over time, as rigid bodies in the world move.",
    "raw_text": "The power of visibility The most powerful paradigm for 6-DOF tracking is visibility. T idea is to identify special parts of the physical world called features and calculate their positio along a line-of-sight ray to a known location. Figure 9.11 shows an example inspired by a came but other hardware could be used. One crucial aspect for tracking is distinguishability. If all features appear to be the same, then it may become difficult to determine and maintain “which is which” during the tracking process. Each feature should be assigned a unique label that is invariant over time, as rigid bodies in the world move.",
    "section_hierarchy": "St. Francis Institute of Technology Department of information Technology > 6. 8. 1 Tracking Position and Orientation > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 614,
    "word_count": 97,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.409593",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b130408"
  },
  {
    "global_chunk_id": "a91be4be-f9ff-4d38-bf50-9ccfc0eb1571",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_95",
    "chunk_index": 95,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 16,
    "bbox": [
      116,
      390,
      882,
      448
    ],
    "text": "The most common sensor used to detect features isadigital camera. Detecting, labeling, and tracking features are common tasks in computer vision or image processing. here are two options for features:",
    "raw_text": "The most common sensor used to detect features is a digital camera. Detecting, labeling, and tracking features are common tasks in computer vision or image processing. here are two options for features:",
    "section_hierarchy": "St. Francis Institute of Technology Department of information Technology > 6. 8. 1 Tracking Position and Orientation > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 200,
    "word_count": 30,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.409593",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b130409"
  },
  {
    "global_chunk_id": "6f8a2a09-049f-4f99-b2fb-9cab7bcea052",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_96",
    "chunk_index": 96,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 17,
    "bbox": [
      116,
      130,
      325,
      147
    ],
    "text": "Academic Year: 2025-26",
    "raw_text": "Academic Year: 2025-26",
    "section_hierarchy": "1. Natural: The features are automatically discovered, assigned labels, and maintained during the tracking process. > 2. Artificial: The features are engineered and placed into the environment so that they can be easily detected, matched to preassigned labels, and tracked. For artificial features, one of the simplest solutions istoprint a special tag onto the object tobetracked. For example, one could print bright red dots onto the object and then scan for their appearance as red blobs in the image. To solve the distinguishability problem, multiple colors, such as red, green, blue, and yellow dots, might be needed. Trouble may occur if these colors exist naturally in other parts of the image. A more reliable method istodesign a specific tag that is clearly distinct from the rest of the image. Such tags can be coded to contain large amounts of information, including a unique identification number. One of the most common coded tags is the QR code. > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 22,
    "word_count": 3,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.410591",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b13040a"
  },
  {
    "global_chunk_id": "965335ff-7f7a-4a1e-a7db-205c1f714fd4",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_97",
    "chunk_index": 97,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 17,
    "bbox": [
      393,
      128,
      633,
      147
    ],
    "text": "Class-BE( INFT) Div- A &B",
    "raw_text": "Class-BE(INFT) Div- A &B",
    "section_hierarchy": "1. Natural: The features are automatically discovered, assigned labels, and maintained during the tracking process. > 2. Artificial: The features are engineered and placed into the environment so that they can be easily detected, matched to preassigned labels, and tracked. For artificial features, one of the simplest solutions istoprint a special tag onto the object tobetracked. For example, one could print bright red dots onto the object and then scan for their appearance as red blobs in the image. To solve the distinguishability problem, multiple colors, such as red, green, blue, and yellow dots, might be needed. Trouble may occur if these colors exist naturally in other parts of the image. A more reliable method istodesign a specific tag that is clearly distinct from the rest of the image. Such tags can be coded to contain large amounts of information, including a unique identification number. One of the most common coded tags is the QR code. > St. Francis Institute of Technology Department of information Technology",
    "heading_context": "St. Francis Institute of Technology Department of information Technology",
    "num_merged_blocks": 1,
    "char_count": 25,
    "word_count": 5,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.411593",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b13040b"
  },
  {
    "global_chunk_id": "2f854430-81c3-41ec-b07f-ce4280bec011",
    "document_id": "Module_6",
    "chunk_id": "Module_6_text_98",
    "chunk_index": 98,
    "chunk_type": "text",
    "source_file": "D:\\Inception.js\\backend\\data\\uploads\\Module_6\\v1\\outputs",
    "page_idx": 17,
    "bbox": [
      111,
      270,
      875,
      520
    ],
    "text": "https://jov. arvojournals. org/article. aspx? articleid=2191910 https://www. aao. org/eye \nhealth/anatomy/photoreceptors#:\\~: tex $\\underline { { \\underline { { \\mathbf { \\Pi } } } } }$ Special%20cells%20in%20the%20eye%27s, problems% 20can%20involve%20photoreceptor%20cells. https://arinsider. co/2021/12/09/vr-resolution-field-of-view-and-the-science-of-the-human-eye/ https://en. wikipedia. org/wiki/List_of_virtual_reality_headsets#/media/File: Resolution_VR_head sets. png https://www. youtube. com/watch? v=bjL-Z7fW2FM",
    "raw_text": "https://jov.arvojournals.org/article.aspx?articleid=2191910 https://www.aao.org/eye  \nhealth/anatomy/photoreceptors#:\\~:tex $\\underline { { \\underline { { \\mathbf { \\Pi } } } } }$ Special%20cells%20in%20the%20eye%27s,problems% 20can%20involve%20photoreceptor%20cells. https://arinsider.co/2021/12/09/vr-resolution-field-of-view-and-the-science-of-the-human-eye/ https://en.wikipedia.org/wiki/List_of_virtual_reality_headsets#/media/File:Resolution_VR_head sets.png https://www.youtube.com/watch?v=bjL-Z7fW2FM",
    "section_hierarchy": "2. Artificial: The features are engineered and placed into the environment so that they can be easily detected, matched to preassigned labels, and tracked. For artificial features, one of the simplest solutions istoprint a special tag onto the object tobetracked. For example, one could print bright red dots onto the object and then scan for their appearance as red blobs in the image. To solve the distinguishability problem, multiple colors, such as red, green, blue, and yellow dots, might be needed. Trouble may occur if these colors exist naturally in other parts of the image. A more reliable method istodesign a specific tag that is clearly distinct from the rest of the image. Such tags can be coded to contain large amounts of information, including a unique identification number. One of the most common coded tags is the QR code. > St. Francis Institute of Technology Department of information Technology > References:",
    "heading_context": "References:",
    "num_merged_blocks": 5,
    "char_count": 523,
    "word_count": 39,
    "has_embedding": false,
    "embedding_model": null,
    "embedding_dimension": null,
    "created_at": "2025-12-14 18:27:59.411593",
    "embedding_created_at": null,
    "_id": "693f01afec9fc9069b13040c"
  }
]